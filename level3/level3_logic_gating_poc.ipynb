{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "543d5396",
   "metadata": {},
   "source": [
    "# Level-3 Neuro-Symbolic AI: Logic-Aware Gating PoC\n",
    "\n",
    "## What are the NSAI Levels?\n",
    "\n",
    "**Level-2 (Pure Statistical Learning)**\n",
    "- The model learns patterns from training data using only statistical correlations\n",
    "- No awareness of logical constraints or domain rules\n",
    "- Can confidently predict logically invalid intent combinations\n",
    "- Example: Predicting `execute` when that action is explicitly forbidden for the current context\n",
    "\n",
    "**Level-2.5 (Post-hoc Logic Filtering)**\n",
    "- The Level-2 model predicts freely\n",
    "- **After** the model produces probabilities, we apply logical constraints:\n",
    "  - Zero out suppressed intents\n",
    "  - Renormalize remaining probabilities\n",
    "- The model's internal representations are unchanged\n",
    "- Logic acts as a post-processing filter, not a learning signal\n",
    "\n",
    "**Level-3 (Logic-Aware Gating)**\n",
    "- Logical constraints are embedded **inside the model's forward pass**\n",
    "- Before the output layer produces final probabilities, a logic gate:\n",
    "  - Receives a mask indicating which intents are allowed/suppressed\n",
    "  - Strongly suppresses logits for forbidden intents **before softmax**\n",
    "  - Ensures invalid intents never become top predictions\n",
    "- The model learns to work **with** constraints, not against them\n",
    "- Logic shapes the prediction formation process itself\n",
    "\n",
    "## Why gating \"inside forward pass\" is Level-3\n",
    "\n",
    "The critical difference:\n",
    "- **L2.5**: Logic corrects `model(x) ‚Üí probs` **after** the fact\n",
    "- **L3**: Logic participates in `model(x, constraints) ‚Üí probs` **during** computation\n",
    "\n",
    "This means:\n",
    "- The model's loss function sees constraint-aware predictions during training\n",
    "- Gradients flow through the gated outputs\n",
    "- The model learns representations that align with logical structure\n",
    "- Invalid predictions are structurally prevented, not just masked\n",
    "\n",
    "This notebook demonstrates all three levels on the same dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e84eb3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Imports complete\n",
      "Canonical intents: ['investigate', 'execute', 'summarize', 'ops']\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Determinism\n",
    "np.random.seed(42)\n",
    "\n",
    "# Canonical intents (do not change)\n",
    "INTENTS = ['investigate', 'execute', 'summarize', 'ops']\n",
    "INTENT_TO_IDX = {intent: i for i, intent in enumerate(INTENTS)}\n",
    "IDX_TO_INTENT = {i: intent for i, intent in enumerate(INTENTS)}\n",
    "\n",
    "print(\"‚úì Imports complete\")\n",
    "print(f\"Canonical intents: {INTENTS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3506e93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded 614 records from c:\\git\\nsai_poc\\level3\\data\\level3_intents.csv\n",
      "\n",
      "Intent distribution after mapping to canonical names:\n",
      "gold_intent\n",
      "out_of_scope    169\n",
      "execute         150\n",
      "investigate     149\n",
      "summarize       146\n",
      "Name: count, dtype: int64\n",
      "\n",
      "‚ö† Filtered out 169 records with non-canonical intents (e.g., 'out_of_scope')\n",
      "\n",
      "‚úì Using 445 records with canonical intents\n",
      "Distribution by canonical intent:\n",
      "  investigate: 149 samples\n",
      "  execute: 150 samples\n",
      "  summarize: 146 samples\n",
      "  ops: 0 samples\n",
      "\n",
      "Columns: ['utterance', 'gold_intent', 'facts', 'active_constraints', 'allowed_intents', 'suppressed_intents']\n",
      "\n",
      "Sample record:\n",
      "  Utterance: why is server host123 cpu high\n",
      "  Gold intent: investigate\n",
      "  Allowed: ['investigate', 'summarize', 'ops']\n",
      "  Suppressed: ['execute']\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 ‚Äî Load Level-3 Dataset\n",
    "\n",
    "def find_repo_root(start_dir=None):\n",
    "    d = start_dir or os.getcwd()\n",
    "    while True:\n",
    "        if os.path.exists(os.path.join(d, 'requirements.txt')) or os.path.exists(os.path.join(d, '.git')):\n",
    "            return d\n",
    "        parent = os.path.dirname(d)\n",
    "        if parent == d:\n",
    "            return os.getcwd()\n",
    "        d = parent\n",
    "\n",
    "repo_root = find_repo_root()\n",
    "sys.path.insert(0, repo_root)\n",
    "\n",
    "# Locate L3 dataset\n",
    "l3_path = None\n",
    "for candidate in [os.path.join(repo_root, 'l3', 'data', 'level3_intents.csv'),\n",
    "                 os.path.join(repo_root, 'level3', 'data', 'level3_intents.csv')]:\n",
    "    if os.path.exists(candidate):\n",
    "        l3_path = candidate\n",
    "        break\n",
    "\n",
    "if l3_path is None:\n",
    "    raise FileNotFoundError('Level-3 dataset not found at l3/data/level3_intents.csv')\n",
    "\n",
    "df = pd.read_csv(l3_path)\n",
    "print(f\"‚úì Loaded {len(df)} records from {l3_path}\")\n",
    "\n",
    "# Map dataset intent names to canonical INTENTS\n",
    "# The data uses 'execution', 'summarization' instead of 'execute', 'summarize'\n",
    "INTENT_MAPPING = {\n",
    "    'investigate': 'investigate',\n",
    "    'execution': 'execute',\n",
    "    'summarization': 'summarize',\n",
    "    'execute': 'execute',  # in case data already uses correct name\n",
    "    'summarize': 'summarize',\n",
    "    # 'out_of_scope' is filtered out (not in canonical INTENTS)\n",
    "    # 'ops' is in canonical but not in current data (that's okay)\n",
    "}\n",
    "\n",
    "# Apply intent mapping to gold_intent\n",
    "df['gold_intent'] = df['gold_intent'].map(lambda x: INTENT_MAPPING.get(x, x))\n",
    "\n",
    "# Check distribution after mapping\n",
    "print(f\"\\nIntent distribution after mapping to canonical names:\")\n",
    "print(df['gold_intent'].value_counts())\n",
    "\n",
    "# Parse list columns safely\n",
    "def parse_list(x):\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return list(x)\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    s = str(x).strip()\n",
    "    if s == '':\n",
    "        return []\n",
    "    try:\n",
    "        v = ast.literal_eval(s)\n",
    "        if isinstance(v, (list, tuple)):\n",
    "            return list(v)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return [item.strip() for item in s.split(',') if item.strip()]\n",
    "\n",
    "def parse_and_map_intents(x):\n",
    "    \"\"\"Parse list and map intent names to canonical.\"\"\"\n",
    "    items = parse_list(x)\n",
    "    return [INTENT_MAPPING.get(item, item) for item in items]\n",
    "\n",
    "for col in ['allowed_intents', 'suppressed_intents']:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(parse_and_map_intents)\n",
    "\n",
    "# Validate dataset\n",
    "required_cols = ['utterance', 'gold_intent', 'allowed_intents', 'suppressed_intents']\n",
    "missing = [c for c in required_cols if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f'Missing required columns: {missing}')\n",
    "\n",
    "# Filter to only canonical intents (after mapping)\n",
    "before_filter = len(df)\n",
    "df = df[df['gold_intent'].isin(INTENTS)].copy()\n",
    "filtered_count = before_filter - len(df)\n",
    "\n",
    "if filtered_count > 0:\n",
    "    print(f\"\\n‚ö† Filtered out {filtered_count} records with non-canonical intents (e.g., 'out_of_scope')\")\n",
    "\n",
    "print(f\"\\n‚úì Using {len(df)} records with canonical intents\")\n",
    "print(f\"Distribution by canonical intent:\")\n",
    "intent_counts = df['gold_intent'].value_counts()\n",
    "for intent in INTENTS:\n",
    "    count = intent_counts.get(intent, 0)\n",
    "    print(f\"  {intent}: {count} samples\")\n",
    "\n",
    "# Ensure allowed_intents is non-empty (use all if empty)\n",
    "df['allowed_intents'] = df['allowed_intents'].apply(\n",
    "    lambda x: x if len(x) > 0 else INTENTS.copy()\n",
    ")\n",
    "\n",
    "# Validation check\n",
    "min_samples_per_class = df['gold_intent'].value_counts().min() if len(df) > 0 else 0\n",
    "if min_samples_per_class == 0:\n",
    "    print(f\"\\n‚ö† WARNING: Some canonical intents have 0 samples!\")\n",
    "    print(f\"   The PoC will work but won't demonstrate all 4 intent classes.\")\n",
    "elif min_samples_per_class < 10:\n",
    "    print(f\"\\n‚ö† WARNING: Minimum samples per class is {min_samples_per_class}\")\n",
    "    print(f\"   This is low but sufficient for PoC demonstration.\")\n",
    "\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nSample record:\")\n",
    "print(f\"  Utterance: {df.iloc[0]['utterance']}\")\n",
    "print(f\"  Gold intent: {df.iloc[0]['gold_intent']}\")\n",
    "print(f\"  Allowed: {df.iloc[0]['allowed_intents']}\")\n",
    "print(f\"  Suppressed: {df.iloc[0]['suppressed_intents']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35fdbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in dataset:\n",
      "  investigate: 149 samples\n",
      "  execute: 150 samples\n",
      "  summarize: 146 samples\n",
      "\n",
      "‚úì Using stratified split\n",
      "\n",
      "Training set class distribution:\n",
      "  investigate: 119 samples\n",
      "  execute: 120 samples\n",
      "  summarize: 117 samples\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sparse array length is ambiguous; use getnnz() or shape[0]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 59\u001b[39m\n\u001b[32m     55\u001b[39m test_df[\u001b[33m'\u001b[39m\u001b[33ml2_pred_intent\u001b[39m\u001b[33m'\u001b[39m] = [IDX_TO_INTENT[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m l2_preds_test]\n\u001b[32m     57\u001b[39m l2_accuracy = (l2_preds_test == y_test).mean()\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚úì L2 baseline trained on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m samples\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     60\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úì Test set: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(X_test)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m samples\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     61\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úì L2 accuracy on test: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml2_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\git\\nsai_poc\\.venv\\Lib\\site-packages\\scipy\\sparse\\_base.py:448\u001b[39m, in \u001b[36m_spbase.__len__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    447\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m448\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33msparse array length is ambiguous; use getnnz()\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    449\u001b[39m                     \u001b[33m\"\u001b[39m\u001b[33m or shape[0]\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: sparse array length is ambiguous; use getnnz() or shape[0]"
     ]
    }
   ],
   "source": [
    "# Cell 2 ‚Äî Minimal Baseline Model (L2)\n",
    "\n",
    "# Simple TF-IDF + Logistic Regression as L2 baseline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Prepare data\n",
    "X_text = df['utterance'].values\n",
    "y_labels = df['gold_intent'].map(INTENT_TO_IDX).values\n",
    "\n",
    "# Check class distribution\n",
    "unique, counts = np.unique(y_labels, return_counts=True)\n",
    "print(f\"Class distribution in dataset:\")\n",
    "for intent_idx, count in zip(unique, counts):\n",
    "    print(f\"  {IDX_TO_INTENT[intent_idx]}: {count} samples\")\n",
    "\n",
    "# Check if we have enough samples for stratified split\n",
    "min_samples = counts.min()\n",
    "use_stratify = min_samples >= 2  # Need at least 2 samples per class for stratified split\n",
    "\n",
    "# Train/test split (80/20)\n",
    "if use_stratify:\n",
    "    X_train_text, X_test_text, y_train, y_test, train_idx, test_idx = train_test_split(\n",
    "        X_text, y_labels, np.arange(len(df)), test_size=0.2, random_state=42, stratify=y_labels\n",
    "    )\n",
    "    print(f\"\\n‚úì Using stratified split\")\n",
    "else:\n",
    "    X_train_text, X_test_text, y_train, y_test, train_idx, test_idx = train_test_split(\n",
    "        X_text, y_labels, np.arange(len(df)), test_size=0.2, random_state=42\n",
    "    )\n",
    "    print(f\"\\n‚ö† Class imbalanced - using random split (not stratified)\")\n",
    "\n",
    "# Vectorize\n",
    "vectorizer = TfidfVectorizer(max_features=500, ngram_range=(1, 2))\n",
    "X_train = vectorizer.fit_transform(X_train_text)\n",
    "X_test = vectorizer.transform(X_test_text)\n",
    "\n",
    "# Check training set class distribution\n",
    "train_unique, train_counts = np.unique(y_train, return_counts=True)\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "for intent_idx, count in zip(train_unique, train_counts):\n",
    "    print(f\"  {IDX_TO_INTENT[intent_idx]}: {count} samples\")\n",
    "\n",
    "# Train L2 baseline model\n",
    "l2_model = LogisticRegression(max_iter=200, random_state=42)\n",
    "l2_model.fit(X_train, y_train)\n",
    "\n",
    "# Get L2 predictions on test set\n",
    "l2_probs_test = l2_model.predict_proba(X_test)  # shape: (n_test, num_classes)\n",
    "l2_preds_test = np.argmax(l2_probs_test, axis=1)\n",
    "\n",
    "# Store in dataframe for later comparison\n",
    "test_df = df.iloc[test_idx].copy().reset_index(drop=True)\n",
    "test_df['l2_probs'] = list(l2_probs_test)\n",
    "test_df['l2_pred_idx'] = l2_preds_test\n",
    "test_df['l2_pred_intent'] = [IDX_TO_INTENT[i] for i in l2_preds_test]\n",
    "\n",
    "l2_accuracy = (l2_preds_test == y_test).mean()\n",
    "\n",
    "print(f\"\\n‚úì L2 baseline trained on {X_train.shape[0]} samples\")\n",
    "print(f\"‚úì Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"‚úì L2 accuracy on test: {l2_accuracy:.2%}\")\n",
    "print(f\"\\nL2 represents: Pure statistical learning with no constraint awareness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d88352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 ‚Äî L2.5 (Post-hoc Masking)\n",
    "\n",
    "def apply_l25_masking(probs: np.ndarray, allowed: List[str], suppressed: List[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply L2.5 post-hoc logic:\n",
    "    - Zero out suppressed intents\n",
    "    - If allowed_intents specified, zero out all others\n",
    "    - Renormalize\n",
    "    \"\"\"\n",
    "    masked_probs = probs.copy()\n",
    "    \n",
    "    # Zero suppressed\n",
    "    for intent in suppressed:\n",
    "        if intent in INTENT_TO_IDX:\n",
    "            masked_probs[INTENT_TO_IDX[intent]] = 0.0\n",
    "    \n",
    "    # Zero non-allowed (if allowed list is not everything)\n",
    "    if set(allowed) != set(INTENTS):\n",
    "        for intent in INTENTS:\n",
    "            if intent not in allowed:\n",
    "                masked_probs[INTENT_TO_IDX[intent]] = 0.0\n",
    "    \n",
    "    # Renormalize\n",
    "    total = masked_probs.sum()\n",
    "    if total > 0:\n",
    "        masked_probs = masked_probs / total\n",
    "    else:\n",
    "        # Fallback: uniform over allowed\n",
    "        masked_probs = np.zeros(len(INTENTS))\n",
    "        for intent in allowed:\n",
    "            if intent in INTENT_TO_IDX:\n",
    "                masked_probs[INTENT_TO_IDX[intent]] = 1.0 / len(allowed)\n",
    "    \n",
    "    return masked_probs\n",
    "\n",
    "# Apply L2.5 to all test predictions\n",
    "l25_probs_list = []\n",
    "for idx, row in test_df.iterrows():\n",
    "    l2_probs = row['l2_probs']\n",
    "    allowed = row['allowed_intents']\n",
    "    suppressed = row['suppressed_intents']\n",
    "    l25_probs = apply_l25_masking(l2_probs, allowed, suppressed)\n",
    "    l25_probs_list.append(l25_probs)\n",
    "\n",
    "l25_probs_array = np.array(l25_probs_list)\n",
    "l25_preds = np.argmax(l25_probs_array, axis=1)\n",
    "\n",
    "test_df['l25_probs'] = list(l25_probs_array)\n",
    "test_df['l25_pred_idx'] = l25_preds\n",
    "test_df['l25_pred_intent'] = [IDX_TO_INTENT[i] for i in l25_preds]\n",
    "\n",
    "l25_accuracy = (l25_preds == y_test).mean()\n",
    "\n",
    "print(f\"‚úì L2.5 post-hoc masking applied to {len(test_df)} test samples\")\n",
    "print(f\"‚úì L2.5 accuracy on test: {l25_accuracy:.2%}\")\n",
    "print(f\"\\nL2.5 represents: Logic applied AFTER model inference (post-processing filter)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4154d8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 ‚Äî Level-3 Model with Logic-Aware Gating\n",
    "\n",
    "class Level3LogicGatedClassifier:\n",
    "    \"\"\"\n",
    "    A simple classifier with logic-aware gating INSIDE the forward pass.\n",
    "    \n",
    "    Architecture:\n",
    "    - TF-IDF features ‚Üí Linear layer ‚Üí Logits\n",
    "    - Logic gate: mask logits BEFORE softmax\n",
    "    - Softmax ‚Üí Final probabilities\n",
    "    \n",
    "    The key difference from L2:\n",
    "    - forward() accepts both features AND allowed_mask\n",
    "    - Gating happens inside forward, not after\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, num_classes: int, mask_value: float = -1e9):\n",
    "        self.num_classes = num_classes\n",
    "        self.mask_value = mask_value\n",
    "        \n",
    "        # Simple linear layer (weights + bias)\n",
    "        self.W = np.random.randn(input_dim, num_classes) * 0.01\n",
    "        self.b = np.zeros(num_classes)\n",
    "    \n",
    "    def forward(self, X, allowed_mask):\n",
    "        \"\"\"\n",
    "        Forward pass with logic-aware gating.\n",
    "        \n",
    "        Args:\n",
    "            X: feature matrix (n_samples, input_dim)\n",
    "            allowed_mask: binary mask (n_samples, num_classes)\n",
    "                         1 = allowed, 0 = suppressed\n",
    "        \n",
    "        Returns:\n",
    "            probs: probability distribution (n_samples, num_classes)\n",
    "        \"\"\"\n",
    "        # Compute logits\n",
    "        if hasattr(X, 'toarray'):  # sparse matrix\n",
    "            X = X.toarray()\n",
    "        logits = X @ self.W + self.b  # shape: (n_samples, num_classes)\n",
    "        \n",
    "        # CRITICAL: Apply logic gate BEFORE softmax\n",
    "        # Mask suppressed intents with large negative value\n",
    "        masked_logits = logits + (1 - allowed_mask) * self.mask_value\n",
    "        \n",
    "        # Softmax (numerically stable)\n",
    "        exp_logits = np.exp(masked_logits - np.max(masked_logits, axis=1, keepdims=True))\n",
    "        probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "        \n",
    "        return probs\n",
    "    \n",
    "    def fit(self, X, y, allowed_masks, epochs=50, lr=0.01):\n",
    "        \"\"\"\n",
    "        Simple gradient descent training.\n",
    "        \n",
    "        Args:\n",
    "            X: features\n",
    "            y: true labels (indices)\n",
    "            allowed_masks: binary masks for each sample\n",
    "            epochs: training iterations\n",
    "            lr: learning rate\n",
    "        \"\"\"\n",
    "        if hasattr(X, 'toarray'):\n",
    "            X = X.toarray()\n",
    "        \n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass with logic gating\n",
    "            probs = self.forward(X, allowed_masks)\n",
    "            \n",
    "            # Compute loss (cross-entropy)\n",
    "            log_probs = np.log(probs + 1e-10)\n",
    "            loss = -np.mean([log_probs[i, y[i]] for i in range(n_samples)])\n",
    "            \n",
    "            # Compute gradients\n",
    "            grad_probs = probs.copy()\n",
    "            for i in range(n_samples):\n",
    "                grad_probs[i, y[i]] -= 1\n",
    "            grad_probs = grad_probs / n_samples\n",
    "            \n",
    "            # Update weights\n",
    "            grad_W = X.T @ grad_probs\n",
    "            grad_b = np.sum(grad_probs, axis=0)\n",
    "            \n",
    "            self.W -= lr * grad_W\n",
    "            self.b -= lr * grad_b\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"  Epoch {epoch}/{epochs}, Loss: {loss:.4f}\")\n",
    "        \n",
    "        print(f\"‚úì Training complete\")\n",
    "\n",
    "# Prepare allowed_masks for training and test\n",
    "def create_allowed_mask(allowed: List[str], suppressed: List[str]) -> np.ndarray:\n",
    "    \"\"\"Create binary mask: 1 for allowed intents, 0 for suppressed.\"\"\"\n",
    "    mask = np.ones(len(INTENTS))\n",
    "    \n",
    "    # Zero out suppressed\n",
    "    for intent in suppressed:\n",
    "        if intent in INTENT_TO_IDX:\n",
    "            mask[INTENT_TO_IDX[intent]] = 0\n",
    "    \n",
    "    # If allowed list specified, zero non-allowed\n",
    "    if set(allowed) != set(INTENTS):\n",
    "        for intent in INTENTS:\n",
    "            if intent not in allowed:\n",
    "                mask[INTENT_TO_IDX[intent]] = 0\n",
    "    \n",
    "    return mask\n",
    "\n",
    "# Build masks for train and test\n",
    "train_df_full = df.iloc[train_idx].copy()\n",
    "train_masks = np.array([\n",
    "    create_allowed_mask(row['allowed_intents'], row['suppressed_intents'])\n",
    "    for _, row in train_df_full.iterrows()\n",
    "])\n",
    "\n",
    "test_masks = np.array([\n",
    "    create_allowed_mask(row['allowed_intents'], row['suppressed_intents'])\n",
    "    for _, row in test_df.iterrows()\n",
    "])\n",
    "\n",
    "# Train L3 model\n",
    "print(\"\\nüîß Training Level-3 model with logic-aware gating...\")\n",
    "l3_model = Level3LogicGatedClassifier(input_dim=X_train.shape[1], num_classes=len(INTENTS))\n",
    "l3_model.fit(X_train, y_train, train_masks, epochs=50, lr=0.1)\n",
    "\n",
    "# Get L3 predictions on test set\n",
    "l3_probs_test = l3_model.forward(X_test, test_masks)\n",
    "l3_preds_test = np.argmax(l3_probs_test, axis=1)\n",
    "\n",
    "test_df['l3_probs'] = list(l3_probs_test)\n",
    "test_df['l3_pred_idx'] = l3_preds_test\n",
    "test_df['l3_pred_intent'] = [IDX_TO_INTENT[i] for i in l3_preds_test]\n",
    "\n",
    "l3_accuracy = (l3_preds_test == y_test).mean()\n",
    "\n",
    "print(f\"\\n‚úì L3 accuracy on test: {l3_accuracy:.2%}\")\n",
    "print(f\"\\nL3 represents: Logic gating INSIDE forward pass (constraint-aware learning)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebca96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 ‚Äî Comparison Metrics (L2 vs L2.5 vs L3)\n",
    "\n",
    "def is_violating(pred_intent: str, allowed: List[str], suppressed: List[str]) -> bool:\n",
    "    \"\"\"Check if predicted intent violates constraints.\"\"\"\n",
    "    if pred_intent in suppressed:\n",
    "        return True\n",
    "    if set(allowed) != set(INTENTS) and pred_intent not in allowed:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Compute violation rates\n",
    "test_df['l2_violates'] = test_df.apply(\n",
    "    lambda row: is_violating(row['l2_pred_intent'], row['allowed_intents'], row['suppressed_intents']),\n",
    "    axis=1\n",
    ")\n",
    "test_df['l25_violates'] = test_df.apply(\n",
    "    lambda row: is_violating(row['l25_pred_intent'], row['allowed_intents'], row['suppressed_intents']),\n",
    "    axis=1\n",
    ")\n",
    "test_df['l3_violates'] = test_df.apply(\n",
    "    lambda row: is_violating(row['l3_pred_intent'], row['allowed_intents'], row['suppressed_intents']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "l2_violation_rate = test_df['l2_violates'].mean()\n",
    "l25_violation_rate = test_df['l25_violates'].mean()\n",
    "l3_violation_rate = test_df['l3_violates'].mean()\n",
    "\n",
    "# Intent flip rates\n",
    "l2_to_l25_flips = (test_df['l2_pred_intent'] != test_df['l25_pred_intent']).sum()\n",
    "l2_to_l3_flips = (test_df['l2_pred_intent'] != test_df['l3_pred_intent']).sum()\n",
    "l25_to_l3_flips = (test_df['l25_pred_intent'] != test_df['l3_pred_intent']).sum()\n",
    "\n",
    "# Gold agreement\n",
    "test_df['l2_correct'] = test_df['l2_pred_intent'] == test_df['gold_intent']\n",
    "test_df['l25_correct'] = test_df['l25_pred_intent'] == test_df['gold_intent']\n",
    "test_df['l3_correct'] = test_df['l3_pred_intent'] == test_df['gold_intent']\n",
    "\n",
    "l2_acc = test_df['l2_correct'].mean()\n",
    "l25_acc = test_df['l25_correct'].mean()\n",
    "l3_acc = test_df['l3_correct'].mean()\n",
    "\n",
    "# Print comparison table\n",
    "print(\"=\"*70)\n",
    "print(\"LEVEL COMPARISON: L2 vs L2.5 vs L3\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTest set size: {len(test_df)} samples\\n\")\n",
    "\n",
    "print(f\"{'Metric':<35} {'L2':>10} {'L2.5':>10} {'L3':>10}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Constraint Violation Rate':<35} {l2_violation_rate:>9.1%} {l25_violation_rate:>9.1%} {l3_violation_rate:>9.1%}\")\n",
    "print(f\"{'Gold Intent Accuracy':<35} {l2_acc:>9.1%} {l25_acc:>9.1%} {l3_acc:>9.1%}\")\n",
    "print(\"\")\n",
    "print(f\"Intent flips from L2 ‚Üí L2.5: {l2_to_l25_flips} ({l2_to_l25_flips/len(test_df):.1%})\")\n",
    "print(f\"Intent flips from L2 ‚Üí L3:   {l2_to_l3_flips} ({l2_to_l3_flips/len(test_df):.1%})\")\n",
    "print(f\"Intent flips from L2.5 ‚Üí L3: {l25_to_l3_flips} ({l25_to_l3_flips/len(test_df):.1%})\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "print(\"\\nüìä Key Observations:\")\n",
    "print(f\"  ‚Ä¢ L2 violation rate shows baseline constraint-unawareness\")\n",
    "print(f\"  ‚Ä¢ L2.5 should have near-zero violations (post-hoc masking)\")\n",
    "print(f\"  ‚Ä¢ L3 should have EXACTLY zero violations (structural prevention)\")\n",
    "print(f\"  ‚Ä¢ Accuracy differences reveal trade-offs between constraint adherence and prediction quality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3fbc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 ‚Äî Side-by-side Examples (Teaching Moment)\n",
    "\n",
    "# Find examples where L2 violated but L2.5/L3 corrected\n",
    "violations = test_df[test_df['l2_violates'] == True].copy()\n",
    "\n",
    "if len(violations) > 0:\n",
    "    print(\"=\"*90)\n",
    "    print(\"CONCRETE EXAMPLES: Where L2 violated constraints\")\n",
    "    print(\"=\"*90)\n",
    "    \n",
    "    # Show first 5 violations (deterministic)\n",
    "    sample = violations.head(5)\n",
    "    \n",
    "    for idx, row in sample.iterrows():\n",
    "        print(f\"\\n{'‚îÄ'*90}\")\n",
    "        print(f\"Utterance: {row['utterance'][:70]}...\" if len(row['utterance']) > 70 else f\"Utterance: {row['utterance']}\")\n",
    "        print(f\"Gold intent: {row['gold_intent']}\")\n",
    "        print(f\"Suppressed: {row['suppressed_intents']}\")\n",
    "        if set(row['allowed_intents']) != set(INTENTS):\n",
    "            print(f\"Allowed: {row['allowed_intents']}\")\n",
    "        print()\n",
    "        \n",
    "        # L2 prediction\n",
    "        l2_probs_dict = {INTENTS[i]: row['l2_probs'][i] for i in range(len(INTENTS))}\n",
    "        l2_top3 = sorted(l2_probs_dict.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "        print(f\"L2 (baseline):\")\n",
    "        print(f\"  Top-1: {row['l2_pred_intent']} {'‚ùå VIOLATES' if row['l2_violates'] else '‚úì'}\")\n",
    "        print(f\"  Probs: {', '.join([f'{k}={v:.3f}' for k, v in l2_top3])}\")\n",
    "        \n",
    "        # L2.5 prediction\n",
    "        l25_probs_dict = {INTENTS[i]: row['l25_probs'][i] for i in range(len(INTENTS))}\n",
    "        l25_top3 = sorted(l25_probs_dict.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "        print(f\"\\nL2.5 (post-hoc masking):\")\n",
    "        print(f\"  Top-1: {row['l25_pred_intent']} {'‚ùå VIOLATES' if row['l25_violates'] else '‚úì fixed'}\")\n",
    "        print(f\"  Probs: {', '.join([f'{k}={v:.3f}' for k, v in l25_top3])}\")\n",
    "        \n",
    "        # L3 prediction\n",
    "        l3_probs_dict = {INTENTS[i]: row['l3_probs'][i] for i in range(len(INTENTS))}\n",
    "        l3_top3 = sorted(l3_probs_dict.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "        print(f\"\\nL3 (logic-aware gating):\")\n",
    "        print(f\"  Top-1: {row['l3_pred_intent']} {'‚ùå VIOLATES' if row['l3_violates'] else '‚úì prevented'}\")\n",
    "        print(f\"  Probs: {', '.join([f'{k}={v:.3f}' for k, v in l3_top3])}\")\n",
    "    \n",
    "    print(f\"\\n{'‚îÄ'*90}\")\n",
    "else:\n",
    "    print(\"\\n‚úì L2 produced no constraint violations on this test set.\")\n",
    "    print(\"(This can happen if the dataset's constraints align naturally with training patterns)\")\n",
    "\n",
    "print(\"\\nüìö Teaching Takeaway:\")\n",
    "print(\"  ‚Ä¢ L2: Can confidently predict INVALID intents (suppressed or disallowed)\")\n",
    "print(\"  ‚Ä¢ L2.5: Fixes violations AFTER prediction by zeroing and renormalizing\")\n",
    "print(\"  ‚Ä¢ L3: Never forms invalid predictions ‚Äî logic gates prevent them structurally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49e0f96",
   "metadata": {},
   "source": [
    "# Level-3 Conclusion\n",
    "\n",
    "## What L3 proved in this PoC\n",
    "\n",
    "**Logic can be embedded inside the model's forward pass.** By applying a logic gate before the softmax activation, we structurally prevent the model from predicting suppressed intents. This is fundamentally different from post-hoc filtering.\n",
    "\n",
    "**L3 violation rate should be exactly zero.** Unlike L2 (which can violate freely) and L2.5 (which corrects after the fact), L3 models cannot produce invalid top-1 predictions by construction. The logic gate masks suppressed logits with large negative values before softmax, ensuring they receive near-zero probability.\n",
    "\n",
    "**The model learns with constraint awareness.** During training, gradients flow through the logic-gated outputs. This means:\n",
    "- The model's loss function only sees valid predictions\n",
    "- The model learns representations that work within the logical constraints\n",
    "- Invalid reasoning paths are not reinforced during learning\n",
    "\n",
    "## What L3 did not prove\n",
    "\n",
    "**This is a minimal PoC, not a production system.** We used:\n",
    "- A simple linear classifier (TF-IDF + logistic regression equivalent)\n",
    "- Basic gradient descent training\n",
    "- Binary masks (allowed/suppressed only)\n",
    "\n",
    "Real Level-3 systems would involve:\n",
    "- More sophisticated architectures (transformers, graph neural networks)\n",
    "- Richer logical constraints (temporal dependencies, multi-step reasoning)\n",
    "- Differentiable logic layers that can learn constraint parameters\n",
    "\n",
    "**We did not prove L3 always improves accuracy.** Constraint enforcement can reduce the model's flexibility, potentially lowering accuracy on edge cases. The trade-off between constraint adherence and predictive performance depends on:\n",
    "- How well constraints align with the true data distribution\n",
    "- Whether the model has enough capacity to learn valid patterns\n",
    "- The quality and coverage of the constraint specifications\n",
    "\n",
    "**We did not demonstrate constraint learning.** In this PoC, constraints were provided as fixed masks. True neuro-symbolic AI might:\n",
    "- Learn constraint parameters from data\n",
    "- Discover latent logical structure\n",
    "- Adapt constraints based on context\n",
    "\n",
    "## Why this is structurally different from L2.5\n",
    "\n",
    "**Timing of logic application:**\n",
    "- **L2.5**: `model(x) ‚Üí raw_probs` ‚Üí `apply_logic(raw_probs) ‚Üí final_probs`\n",
    "- **L3**: `model(x, constraints) ‚Üí final_probs` (logic inside forward)\n",
    "\n",
    "**Gradient flow:**\n",
    "- **L2.5**: Gradients flow through unconstrained predictions; logic is a non-differentiable post-process\n",
    "- **L3**: Gradients flow through constrained predictions; logic participates in learning\n",
    "\n",
    "**Representation learning:**\n",
    "- **L2.5**: Model learns features without constraint awareness; may waste capacity on invalid patterns\n",
    "- **L3**: Model learns features that align with constraints; representations are structurally informed by logic\n",
    "\n",
    "**Architectural commitment:**\n",
    "- **L2.5**: Logic is external; can be added/removed without retraining\n",
    "- **L3**: Logic is embedded; model architecture explicitly includes constraint handling\n",
    "\n",
    "---\n",
    "\n",
    "## Final Verdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991fc48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 ‚Äî Final Verdict\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LEVEL-3 POC COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"We demonstrated:\")\n",
    "print(\"  ‚úì L2: Pure statistical learning (no constraint awareness)\")\n",
    "print(\"  ‚úì L2.5: Post-hoc logic filtering (constraints applied AFTER inference)\")\n",
    "print(\"  ‚úì L3: Logic-aware gating (constraints embedded INSIDE forward pass)\")\n",
    "print()\n",
    "print(\"Key architectural distinction:\")\n",
    "print(\"  L2.5 corrects invalid outputs after they form\")\n",
    "print(\"  L3 prevents invalid outputs from forming\")\n",
    "print()\n",
    "print(\"This is the foundation of neuro-symbolic AI:\")\n",
    "print(\"  Logic is not a post-processing step\")\n",
    "print(\"  Logic is a structural component of the model\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
