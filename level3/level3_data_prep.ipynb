{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e096b24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Level-3 Data Preparation — Design + Deterministic Code Skeleton\n",
    "#\n",
    "# Purpose:\n",
    "#   Deterministically convert Level-2 training examples (utterance + intent)\n",
    "#   into a Level-3, reasoning-ready record with explicit symbolic facts,\n",
    "#   activated constraints, and a derived intent possibility space.\n",
    "#\n",
    "# Design principles (compiler-style):\n",
    "# - Deterministic: keyword tables, regexes, and simple syntactic heuristics.\n",
    "# - Explainable: every fact and constraint is rule-based and auditable.\n",
    "# - Reproducible: same input -> same output.\n",
    "# - Non-probabilistic: no LLMs, no statistical inference, no randomness.\n",
    "#\n",
    "# Assumptions about input rows: a mapping-like object with keys:\n",
    "#   - 'utterance' (str)\n",
    "#   - 'intent' (one of: 'investigate','execute','summarize','ops')\n",
    "#\n",
    "# Target output schema (per record):\n",
    "#   {\n",
    "#     'utterance': <str>,\n",
    "#     'gold_intent': <str>,\n",
    "#     'facts': { ... boolean / categorical facts ... },\n",
    "#     'active_constraints': [ ... symbolic constraints ... ],\n",
    "#     'allowed_intents': [ ... ],\n",
    "#     'suppressed_intents': [ ... ]\n",
    "#   }\n",
    "#\n",
    "# ------------------------------------------------------------\n",
    "import re\n",
    "from typing import Dict, List, Mapping, Any, Tuple\n",
    "# Canonical intent space (do not add or remove intents)\n",
    "INTENT_SPACE = ['investigate', 'execute', 'summarize', 'ops']\n",
    "# Keyword tables and patterns used by the deterministic extractor.\n",
    "QUESTION_WORDS = ['who','what','when','where','why','how']\n",
    "IMPERATIVE_VERBS = [\n",
    "    'restart','reboot','delete','remove','kill','terminate','scale','deploy',\n",
    "    'push','apply','update','upgrade','rollback','start','stop','reconcile',\n",
    "    'reconfigure','clean','clear','fix','patch'\n",
    "]\n",
    "UNCERTAINTY_MARKERS = ['maybe','might','could','possibly','probably','not sure','unclear','uncertain']\n",
    "TEMPORAL_PATTERNS = [\n",
    "    # simple regex fragments; used with re.IGNORECASE\n",
    "    r'\\b(now|immediately|asap|right now)\\b',\n",
    "    r'\\b(last|past|previous|since)\\b',\n",
    "    r'\\b(in the last|for the last|over the last)\\b',\n",
    "    r'\\b(\\d+\\s?(s|m|h|d|minutes?|hours?|days?))\\b',\n",
    "    r\"\\b(today|yesterday|tomorrow|this morning|this afternoon|this evening)\\b\"\n",
    "]\n",
    "SYSTEM_STATE_KEYWORDS = ['cpu','memory','latency','error','fail','crash','down','status','uptime','spike','throttle','overloaded','slow','leak']\n",
    "ACTION_KEYWORDS = IMPERATIVE_VERBS + ['check','show','diagnose','investigate','explain','report','summarize']\n",
    "# Precompile regexes\n",
    "TEMPORAL_REGEXES = [re.compile(pat, re.IGNORECASE) for pat in TEMPORAL_PATTERNS]\n",
    "QUESTION_REGEX = re.compile(r\"\\?\\s*$\")  # trailing question mark heuristic\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def extract_facts(utterance: str) -> Dict[str, Any]:\n",
    "    \"\"\"Deterministically extract observable facts from an utterance.\n",
    "\n",
    "    Rules and heuristics used (explicit):\n",
    "\n",
    "    - is_question: True if the utterance ends with a question mark OR begins with a question word.\n",
    "\n",
    "    - has_imperative: True if the first token is an imperative verb from a canonical table OR the\n",
    "      utterance begins with an imperative verb (case-insensitive).\n",
    "\n",
    "    - has_uncertainty_marker: True if any token/phrase from the uncertainty markers table appears.\n",
    "\n",
    "    - has_temporal_reference: True if any simple temporal regex matches.\n",
    "\n",
    "    - mentions_action: True if action keywords appear anywhere (deterministic keyword match).\n",
    "\n",
    "    - mentions_system_state: True if system-state keywords appear anywhere.\n",
    "\n",
    "    The function returns a facts dict composed of boolean flags and small categorical fields.\n",
    "\n",
    "    Deterministic: only exact keyword matches, simple regexes, and token-position heuristics.\n",
    "\n",
    "    \"\"\"\n",
    "    if utterance is None:\n",
    "        utterance = ''\n",
    "    u = utterance.strip()\n",
    "    u_lower = u.lower()\n",
    "    tokens = re.findall(r\"\\w+\", u_lower)\n",
    "    # is_question\n",
    "    starts_with_qword = len(tokens) > 0 and tokens[0] in QUESTION_WORDS\n",
    "    ends_with_qmark = bool(QUESTION_REGEX.search(u))\n",
    "    is_question = starts_with_qword or ends_with_qmark\n",
    "    # has_imperative: check first token against imperative verbs table\n",
    "    has_imperative = False\n",
    "    if len(tokens) > 0 and tokens[0] in IMPERATIVE_VERBS:\n",
    "        has_imperative = True\n",
    "    # Also consider utterances that begin with a known action verb phrase (two-word checks)\n",
    "    if not has_imperative:\n",
    "        first_two = ' '.join(tokens[:2])\n",
    "        if any(first_two.startswith(v) for v in IMPERATIVE_VERBS):\n",
    "            has_imperative = True\n",
    "    # uncertainty markers\n",
    "    has_uncertainty_marker = any(marker in u_lower for marker in UNCERTAINTY_MARKERS)\n",
    "    # temporal references\n",
    "    has_temporal_reference = any(rx.search(u) for rx in TEMPORAL_REGEXES)\n",
    "    # mentions_action/system_state\n",
    "    mentions_action = any(word in u_lower for word in ACTION_KEYWORDS)\n",
    "    mentions_system_state = any(word in u_lower for word in SYSTEM_STATE_KEYWORDS)\n",
    "    # Provide minimal explainable categorical facts if helpful\n",
    "    question_word = tokens[0] if starts_with_qword else None\n",
    "    imperative_verb = tokens[0] if has_imperative else None\n",
    "    facts = {\n",
    "        'is_question': bool(is_question),\n",
    "        'question_word': question_word,\n",
    "        'has_imperative': bool(has_imperative),\n",
    "        'imperative_verb': imperative_verb,\n",
    "        'has_uncertainty_marker': bool(has_uncertainty_marker),\n",
    "        'has_temporal_reference': bool(has_temporal_reference),\n",
    "        'mentions_action': bool(mentions_action),\n",
    "        'mentions_system_state': bool(mentions_system_state),\n",
    "    }\n",
    "    return facts\n",
    "\n",
    "\n",
    "def activate_constraints(facts: Mapping[str, Any]) -> List[str]:\n",
    "    \"\"\"Deterministically activate symbolic constraints based on extracted facts.\n",
    "\n",
    "    Constraints are explicit, testable, and explainable. Example mapping rules:\n",
    "\n",
    "    - is_question -> 'block_execute' (questions should not trigger direct executes)\n",
    "\n",
    "    - has_uncertainty_marker -> 'prefer_investigate' (bias towards investigation)\n",
    "\n",
    "    - has_imperative -> 'allow_execute' (imperative language signals operational action)\n",
    "\n",
    "    - mentions_system_state and is_question -> 'require_diagnostic_investigate'\n",
    "\n",
    "    - has_temporal_reference with imperative -> 'require_immediacy_flag' (informational only)\n",
    "\n",
    "    Each rule is a boolean test over facts; output is a list of symbolic constraint strings.\n",
    "\n",
    "    \"\"\"\n",
    "    constraints: List[str] = []\n",
    "    if facts.get('is_question'):\n",
    "        constraints.append('block_execute')\n",
    "    if facts.get('has_uncertainty_marker'):\n",
    "        constraints.append('prefer_investigate')\n",
    "    if facts.get('has_imperative'):\n",
    "        constraints.append('allow_execute')\n",
    "    if facts.get('mentions_system_state') and facts.get('is_question'):\n",
    "        constraints.append('require_diagnostic_investigate')\n",
    "    if facts.get('has_temporal_reference') and facts.get('has_imperative'):\n",
    "        constraints.append('require_immediacy_flag')\n",
    "    # Ensure constraints are unique and ordered deterministically\n",
    "    unique_constraints = []\n",
    "    for c in constraints:\n",
    "        if c not in unique_constraints:\n",
    "            unique_constraints.append(c)\n",
    "    return unique_constraints\n",
    "\n",
    "\n",
    "def derive_intent_space(constraints: List[str]) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"Deterministically derive allowed and suppressed intents from active constraints.\n",
    "\n",
    "    Principles (explicit):\n",
    "\n",
    "    - Start with the full canonical intent set as allowed.\n",
    "\n",
    "    - Apply constraints in a fixed order to remove (suppress) or re-allow intents.\n",
    "\n",
    "    - The function returns (allowed_intents, suppressed_intents) as lists.\n",
    "\n",
    "    Rules (examples):\n",
    "\n",
    "    - 'block_execute' -> remove 'execute' from allowed (suppress execute)\n",
    "\n",
    "    - 'prefer_investigate' -> ensure 'investigate' is in allowed (no-op if already)\n",
    "\n",
    "    - 'allow_execute' -> ensure 'execute' is in allowed (if conflicting, allow_execute wins only if block_execute absent)\n",
    "\n",
    "    - 'require_diagnostic_investigate' -> add 'investigate' and suppress 'execute'\n",
    "\n",
    "    Note: This function is entirely symbolic and deterministic.\n",
    "\n",
    "    \"\"\"\n",
    "    allowed = list(INTENT_SPACE)  # deterministic copy\n",
    "    suppressed: List[str] = []\n",
    "    # Apply rules in a deterministic sequence\n",
    "    if 'block_execute' in constraints:\n",
    "        if 'execute' in allowed:\n",
    "            allowed.remove('execute')\n",
    "            suppressed.append('execute')\n",
    "    if 'require_diagnostic_investigate' in constraints:\n",
    "        # ensure investigate present\n",
    "        if 'investigate' not in allowed:\n",
    "            allowed.append('investigate')\n",
    "        if 'execute' in allowed:\n",
    "            allowed.remove('execute')\n",
    "            if 'execute' not in suppressed:\n",
    "                suppressed.append('execute')\n",
    "    if 'prefer_investigate' in constraints:\n",
    "        if 'investigate' not in allowed:\n",
    "            allowed.append('investigate')\n",
    "    # allow_execute only re-allows execute when block_execute is absent\n",
    "    if 'allow_execute' in constraints and 'block_execute' not in constraints:\n",
    "        if 'execute' not in allowed:\n",
    "            allowed.append('execute')\n",
    "        if 'execute' in suppressed:\n",
    "            suppressed.remove('execute')\n",
    "    # Final deterministic ordering\n",
    "    allowed_sorted = [i for i in INTENT_SPACE if i in allowed]\n",
    "    suppressed_sorted = [i for i in INTENT_SPACE if i in suppressed]\n",
    "    return allowed_sorted, suppressed_sorted\n",
    "\n",
    "\n",
    "def compile_l3_record(row: Mapping[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Compile a single Level-3 record from an input row.\n",
    "\n",
    "    Expected input keys: 'utterance' (str), 'intent' (gold label).\n",
    "\n",
    "    Returns a dict following the Level-3 schema described above.\n",
    "\n",
    "    This function performs only deterministic transformations and does NOT mutate\n",
    "    the original gold intent label.\n",
    "\n",
    "    \"\"\"\n",
    "    utterance = (row.get('utterance') or '').strip()\n",
    "    gold_intent = row.get('intent')\n",
    "    facts = extract_facts(utterance)\n",
    "    active_constraints = activate_constraints(facts)\n",
    "    allowed_intents, suppressed_intents = derive_intent_space(active_constraints)\n",
    "    record = {\n",
    "        'utterance': utterance,\n",
    "        'gold_intent': gold_intent,\n",
    "        'facts': facts,\n",
    "        'active_constraints': active_constraints,\n",
    "        'allowed_intents': allowed_intents,\n",
    "        'suppressed_intents': suppressed_intents,\n",
    "    }\n",
    "    return record\n",
    "\n",
    "# End of cell: deterministic, auditable, Python-only skeleton ready for use in later cells.\n",
    "# Note: This cell contains no execution statements, prints, or randomness. It is safe\n",
    "# to import into downstream notebooks for deterministic Level-3 dataset compilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30573b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Wrote 614 Level-3 records to: c:\\git\\nsai_poc\\level3\\data\\level3_intents.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 — Compile Level-3 dataset and save to level3/data (runnable)\n",
    "import os\n",
    "import pandas as pd\n",
    "from typing import Any, Mapping\n",
    "\n",
    "# Robust repo root finder (replicated to be self-contained in this cell)\n",
    "def find_repo_root(start_dir=None):\n",
    "    d = start_dir or os.getcwd()\n",
    "    while True:\n",
    "        if os.path.exists(os.path.join(d, 'requirements.txt')) or os.path.exists(os.path.join(d, '.git')):\n",
    "            return d\n",
    "        parent = os.path.dirname(d)\n",
    "        if parent == d:\n",
    "            return os.getcwd()\n",
    "        d = parent\n",
    "\n",
    "repo_root = find_repo_root()\n",
    "input_path = os.path.join(repo_root, 'data', 'intents_base.csv')\n",
    "\n",
    "if not os.path.exists(input_path):\n",
    "    raise FileNotFoundError(f\"Expected input CSV not found at {input_path}\")\n",
    "\n",
    "# Load input CSV (expects columns: 'utterance', 'intent')\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# Validate required columns\n",
    "if 'utterance' not in df.columns or 'intent' not in df.columns:\n",
    "    raise ValueError(\"Input CSV must contain 'utterance' and 'intent' columns\")\n",
    "\n",
    "# Compile Level-3 records deterministically using compile_l3_record from Cell 1\n",
    "records = []\n",
    "for _, row in df.iterrows():\n",
    "    # Use a plain mapping to avoid pandas-specific behavior\n",
    "    mapping: Mapping[str, Any] = {'utterance': row['utterance'], 'intent': row['intent']}\n",
    "    rec = compile_l3_record(mapping)\n",
    "    records.append(rec)\n",
    "\n",
    "out_df = pd.DataFrame(records)\n",
    "\n",
    "# Ensure level3/data exists and save\n",
    "out_dir = os.path.join(repo_root, 'level3', 'data')\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "out_path = os.path.join(out_dir, 'level3_intents.csv')\n",
    "out_df.to_csv(out_path, index=False)\n",
    "\n",
    "print(f\"✓ Wrote {len(out_df)} Level-3 records to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "358da2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 — Constraint Taxonomy & Governance (declarative, runnable)\n",
    "# canonical registry of constraints used by Level-3 reasoning\n",
    "CONSTRAINT_REGISTRY = {\n",
    "    'block_execute': {\n",
    "        'type': 'hard',\n",
    "        'description': 'Prevent direct execution when present',\n",
    "        'rationale': 'Questions/clarifications should not trigger executes'\n",
    "    },\n",
    "    'prefer_investigate': {\n",
    "        'type': 'soft',\n",
    "        'description': 'Prefer investigation over execution',\n",
    "        'rationale': 'Uncertainty markers bias toward investigative actions'\n",
    "    },\n",
    "    'allow_execute': {\n",
    "        'type': 'soft',\n",
    "        'description': 'Allow execution when imperative language is detected',\n",
    "        'rationale': 'Imperative phrasing indicates operator intent'\n",
    "    },\n",
    "    'require_diagnostic_investigate': {\n",
    "        'type': 'hard',\n",
    "        'description': 'Require a diagnostic investigation for system-state questions',\n",
    "        'rationale': 'System-state inquiries require diagnostics before action'\n",
    "    },\n",
    "    'require_immediacy_flag': {\n",
    "        'type': 'soft',\n",
    "        'description': 'Mark request as immediate/urgent when temporal + imperative',\n",
    "        'rationale': 'Communicates urgency for downstream handling'\n",
    "    }\n",
    "}\n",
    "\n",
    "# This registry is declarative, importable, and intentionally computation-free.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca39e813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Sanity Checks\n",
      "Total records: 614\n",
      "gold_intent not in allowed_intents: 169\n",
      "gold_intent in suppressed_intents: 0\n",
      "allowed_intents empty: 0\n",
      "\n",
      "Sample violating rows (up to 10):\n",
      "                    utterance  gold_intent                                allowed_intents suppressed_intents active_constraints\n",
      "            hello how are you out_of_scope ['investigate', 'execute', 'summarize', 'ops']                 []                 []\n",
      "what is the capital of france out_of_scope            ['investigate', 'summarize', 'ops']        ['execute']  ['block_execute']\n",
      "               tell me a joke out_of_scope ['investigate', 'execute', 'summarize', 'ops']                 []                 []\n",
      "        who won the world cup out_of_scope            ['investigate', 'summarize', 'ops']        ['execute']  ['block_execute']\n",
      "   write a poem about servers out_of_scope ['investigate', 'execute', 'summarize', 'ops']                 []                 []\n",
      "     how is the weather today out_of_scope            ['investigate', 'summarize', 'ops']        ['execute']  ['block_execute']\n",
      "              what time is it out_of_scope            ['investigate', 'summarize', 'ops']        ['execute']  ['block_execute']\n",
      "      tell me about astronomy out_of_scope ['investigate', 'execute', 'summarize', 'ops']                 []                 []\n",
      "         who is the president out_of_scope            ['investigate', 'summarize', 'ops']        ['execute']  ['block_execute']\n",
      "  what is the meaning of life out_of_scope            ['investigate', 'summarize', 'ops']        ['execute']  ['block_execute']\n",
      "WARNING: Too many logical violations: 169 > 6 — proceeding (review required)\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 — Dataset Sanity & Logical Consistency Checks\n",
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "from typing import Any\n",
    "\n",
    "# load compiled level3 CSV\n",
    "repo_root = os.getcwd()\n",
    "for _ in range(6):\n",
    "    if os.path.exists(os.path.join(repo_root, 'requirements.txt')) or os.path.exists(os.path.join(repo_root, '.git')):\n",
    "        break\n",
    "    repo_root = os.path.dirname(repo_root)\n",
    "level3_path = os.path.join(repo_root, 'level3', 'data', 'level3_intents.csv')\n",
    "if not os.path.exists(level3_path):\n",
    "    raise FileNotFoundError(level3_path)\n",
    "\n",
    "df = pd.read_csv(level3_path)\n",
    "\n",
    "# safe parser for list-like cells\n",
    "def _parse_list_cell(x: Any):\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return list(x)\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    if isinstance(x, str):\n",
    "        x = x.strip()\n",
    "        if x == '':\n",
    "            return []\n",
    "        try:\n",
    "            val = ast.literal_eval(x)\n",
    "            if isinstance(val, (list, tuple)):\n",
    "                return list(val)\n",
    "        except Exception:\n",
    "            return [x]\n",
    "    return []\n",
    "\n",
    "# Normalize expected list-columns\n",
    "for c in ['allowed_intents','suppressed_intents','active_constraints']:\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].apply(_parse_list_cell)\n",
    "\n",
    "# checks\n",
    "if 'gold_intent' not in df.columns:\n",
    "    raise ValueError('gold_intent column missing')\n",
    "\n",
    "# conservative normalization for checking only (prefix-based rules)\n",
    "def _normalize_intent_for_check(intent: Any):\n",
    "    if not isinstance(intent, str):\n",
    "        return intent\n",
    "    k = intent.strip().lower()\n",
    "    if k in INTENT_SPACE:\n",
    "        return k\n",
    "    if k.startswith('execut'):\n",
    "        return 'execute'\n",
    "    if k.startswith('summar'):\n",
    "        return 'summarize'\n",
    "    if k.startswith('investig'):\n",
    "        return 'investigate'\n",
    "    if k.startswith('op'):\n",
    "        return 'ops'\n",
    "    return k\n",
    "\n",
    "\n",
    "def _gold_not_allowed(row):\n",
    "    gi = _normalize_intent_for_check(row['gold_intent'])\n",
    "    ai = [a.strip().lower() for a in (row.get('allowed_intents') or [])]\n",
    "    return gi not in ai\n",
    "\n",
    "def _gold_in_suppressed(row):\n",
    "    gi = _normalize_intent_for_check(row['gold_intent'])\n",
    "    si = [s.strip().lower() for s in (row.get('suppressed_intents') or [])]\n",
    "    return gi in si\n",
    "\n",
    "def _empty_allowed(row):\n",
    "    ai = row.get('allowed_intents') or []\n",
    "    return len(ai) == 0\n",
    "\n",
    "viol_gold_not_allowed = df.apply(_gold_not_allowed, axis=1)\n",
    "viol_gold_in_suppressed = df.apply(_gold_in_suppressed, axis=1)\n",
    "viol_empty_allowed = df.apply(_empty_allowed, axis=1)\n",
    "\n",
    "count_not_allowed = int(viol_gold_not_allowed.sum())\n",
    "count_in_suppressed = int(viol_gold_in_suppressed.sum())\n",
    "count_empty_allowed = int(viol_empty_allowed.sum())\n",
    "\n",
    "total = len(df)\n",
    "print('Dataset Sanity Checks')\n",
    "print('Total records:', total)\n",
    "print(f\"gold_intent not in allowed_intents: {count_not_allowed}\")\n",
    "print(f\"gold_intent in suppressed_intents: {count_in_suppressed}\")\n",
    "print(f\"allowed_intents empty: {count_empty_allowed}\")\n",
    "\n",
    "# small sample of violating rows (if any)\n",
    "viol_any = df[viol_gold_not_allowed | viol_gold_in_suppressed | viol_empty_allowed]\n",
    "if len(viol_any) > 0:\n",
    "    print('\\nSample violating rows (up to 10):')\n",
    "    cols = ['utterance','gold_intent','allowed_intents','suppressed_intents','active_constraints']\n",
    "    cols = [c for c in cols if c in df.columns]\n",
    "    sample = viol_any[cols].head(10).copy()\n",
    "    for c in ['allowed_intents','suppressed_intents','active_constraints']:\n",
    "        if c in sample.columns:\n",
    "            sample[c] = sample[c].apply(lambda v: repr(v))\n",
    "    print(sample.to_string(index=False))\n",
    "\n",
    "# fail-fast threshold (configurable); if exceeded, report WARNING but do not raise\n",
    "threshold = max(5, int(0.01 * total))\n",
    "if len(viol_any) > threshold:\n",
    "    print(f\"WARNING: Too many logical violations: {len(viol_any)} > {threshold} — proceeding (review required)\")\n",
    "\n",
    "# expose counts and pass/fail flag\n",
    "SANITY_COUNTS = {\n",
    "    'total': int(total),\n",
    "    'gold_not_allowed': count_not_allowed,\n",
    "    'gold_in_suppressed': count_in_suppressed,\n",
    "    'empty_allowed': count_empty_allowed,\n",
    "}\n",
    "SANITY_OK = len(viol_any) == 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e00bdaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constraint frequencies (constraint: count)\n",
      "   : 17\n",
      "  ': 522\n",
      "  ,: 17\n",
      "  [: 614\n",
      "  ]: 614\n",
      "  _: 278\n",
      "  a: 99\n",
      "  b: 179\n",
      "  c: 440\n",
      "  d: 17\n",
      "  e: 800\n",
      "  g: 34\n",
      "  i: 85\n",
      "  k: 179\n",
      "  l: 309\n",
      "  n: 34\n",
      "  o: 261\n",
      "  q: 17\n",
      "  r: 34\n",
      "  s: 34\n",
      "  t: 295\n",
      "  u: 261\n",
      "  v: 17\n",
      "  w: 65\n",
      "  x: 244\n",
      "\n",
      "Suppressed intent frequencies (intent: count)\n",
      "  ': 358\n",
      "  [: 614\n",
      "  ]: 614\n",
      "  c: 179\n",
      "  e: 537\n",
      "  t: 179\n",
      "  u: 179\n",
      "  x: 179\n",
      "\n",
      "Top constraint combinations (combo -> count)\n",
      "  ['[', ']'] -> 370\n",
      "  ['[', \"'\", 'b', 'l', 'o', 'c', 'k', '_', 'e', 'x', 'e', 'c', 'u', 't', 'e', \"'\", ']'] -> 162\n",
      "  ['[', \"'\", 'a', 'l', 'l', 'o', 'w', '_', 'e', 'x', 'e', 'c', 'u', 't', 'e', \"'\", ']'] -> 65\n",
      "  ['[', \"'\", 'b', 'l', 'o', 'c', 'k', '_', 'e', 'x', 'e', 'c', 'u', 't', 'e', \"'\", ',', ' ', \"'\", 'r', 'e', 'q', 'u', 'i', 'r', 'e', '_', 'd', 'i', 'a', 'g', 'n', 'o', 's', 't', 'i', 'c', '_', 'i', 'n', 'v', 'e', 's', 't', 'i', 'g', 'a', 't', 'e', \"'\", ']'] -> 17\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 — Coverage & Distribution Diagnostics\n",
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "from collections import Counter\n",
    "from typing import Any\n",
    "\n",
    "# load compiled level3 CSV\n",
    "repo_root = os.getcwd()\n",
    "for _ in range(6):\n",
    "    if os.path.exists(os.path.join(repo_root, 'requirements.txt')) or os.path.exists(os.path.join(repo_root, '.git')):\n",
    "        break\n",
    "    repo_root = os.path.dirname(repo_root)\n",
    "level3_path = os.path.join(repo_root, 'level3', 'data', 'level3_intents.csv')\n",
    "if not os.path.exists(level3_path):\n",
    "    raise FileNotFoundError(level3_path)\n",
    "\n",
    "df = pd.read_csv(level3_path)\n",
    "\n",
    "# safe parser for list-like cells (reuse same logic as sanity cell)\n",
    "def _parse_list_cell(x: Any):\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return list(x)\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    if isinstance(x, str):\n",
    "        x = x.strip()\n",
    "        if x == '':\n",
    "            return []\n",
    "        try:\n",
    "            val = ast.literal_eval(x)\n",
    "            if isinstance(val, (list, tuple)):\n",
    "                return list(val)\n",
    "        except Exception:\n",
    "            return [x]\n",
    "    return []\n",
    "\n",
    "for c in ['active_constraints','suppressed_intents','allowed_intents']:\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].apply(_parse_list_cell)\n",
    "\n",
    "# frequency of each constraint\n",
    "all_constraints = []\n",
    "for row in df['active_constraints']:\n",
    "    all_constraints.extend(row)\n",
    "constraint_counts = Counter(all_constraints)\n",
    "\n",
    "print('Constraint frequencies (constraint: count)')\n",
    "for k,v in sorted(constraint_counts.items()):\n",
    "    print(f'  {k}: {v}')\n",
    "\n",
    "# how often each intent is suppressed\n",
    "suppressed_list = []\n",
    "if 'suppressed_intents' in df.columns:\n",
    "    for s in df['suppressed_intents']:\n",
    "        suppressed_list.extend(s)\n",
    "suppressed_counts = Counter(suppressed_list)\n",
    "print('\\nSuppressed intent frequencies (intent: count)')\n",
    "for k,v in sorted(suppressed_counts.items()):\n",
    "    print(f'  {k}: {v}')\n",
    "\n",
    "# common constraint combinations (deterministic ordering)\n",
    "combo_counts = Counter()\n",
    "for row in df['active_constraints']:\n",
    "    combo = tuple(row)  # keep order as-is (deterministic)\n",
    "    combo_counts[combo] += 1\n",
    "\n",
    "print('\\nTop constraint combinations (combo -> count)')\n",
    "for combo, cnt in combo_counts.most_common(10):\n",
    "    print(f'  {list(combo)} -> {cnt}')\n",
    "\n",
    "# expose summaries\n",
    "CONSTRAINT_FREQUENCIES = dict(sorted(constraint_counts.items()))\n",
    "SUPPRESSED_INTENT_FREQUENCIES = dict(sorted(suppressed_counts.items()))\n",
    "COMMON_CONSTRAINT_COMBOS = [(list(c), n) for c,n in combo_counts.most_common(10)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7f782d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 — Level-3 Readiness Summary (final, lightweight)\n",
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# locate file\n",
    "repo_root = os.getcwd()\n",
    "# try to find repo root similarly to previous cells\n",
    "for _ in range(6):\n",
    "    if os.path.exists(os.path.join(repo_root, 'requirements.txt')) or os.path.exists(os.path.join(repo_root, '.git')):\n",
    "        break\n",
    "    repo_root = os.path.dirname(repo_root)\n",
    "level3_path = os.path.join(repo_root, 'level3', 'data', 'level3_intents.csv')\n",
    "if not os.path.exists(level3_path):\n",
    "    raise FileNotFoundError(f\"Level-3 file not found at {level3_path}\")\n",
    "\n",
    "df = pd.read_csv(level3_path)\n",
    "# parse list/dict-like columns if saved as strings\n",
    "for c in ['active_constraints','allowed_intents','suppressed_intents']:\n",
    "    if c in df.columns:\n",
    "        if df[c].dtype == object:\n",
    "            def _parse(x):\n",
    "                if pd.isna(x):\n",
    "                    return []\n",
    "                if isinstance(x, (list, tuple)):\n",
    "                    return x\n",
    "                try:\n",
    "                    return ast.literal_eval(x)\n",
    "                except Exception:\n",
    "                    return []\n",
    "            df[c] = df[c].apply(_parse)\n",
    "\n",
    "# helper: detect hard constraints using registry if available\n",
    "try:\n",
    "    from level3.level3_data_prep import CONSTRAINT_REGISTRY  # avoid circular import; falls back\n",
    "except Exception:\n",
    "    # try to import from this notebook's namespace if present\n",
    "    CONSTRAINT_REGISTRY = globals().get('CONSTRAINT_REGISTRY', None)\n",
    "\n",
    "hard_set = set()\n",
    "if isinstance(CONSTRAINT_REGISTRY, dict):\n",
    "    hard_set = {k for k,v in CONSTRAINT_REGISTRY.items() if v.get('type') == 'hard'}\n",
    "\n",
    "# compute readiness metrics\n",
    "total = len(df)\n",
    "rows_with_hard = 0\n",
    "rows_with_suppressed = 0\n",
    "for _, r in df.iterrows():\n",
    "    ac = r.get('active_constraints') or []\n",
    "    if any((c in hard_set) for c in ac):\n",
    "        rows_with_hard += 1\n",
    "    si = r.get('suppressed_intents') or []\n",
    "    if len(si) > 0:\n",
    "        rows_with_suppressed += 1\n",
    "\n",
    "pct_hard = rows_with_hard / total * 100 if total else 0.0\n",
    "pct_suppressed = rows_with_suppressed / total * 100 if total else 0.0\n",
    "\n",
    "# recompute logical violations (as defensive check)\n",
    "import ast as _ast\n",
    "\n",
    "def _parse_list_cell(x):\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return x\n",
    "    try:\n",
    "        return _ast.literal_eval(x)\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "if 'allowed_intents' in df.columns and 'suppressed_intents' in df.columns and 'gold_intent' in df.columns:\n",
    "    allowed = df['allowed_intents'].apply(_parse_list_cell)\n",
    "    suppressed = df['suppressed_intents'].apply(_parse_list_cell)\n",
    "    gold = df['gold_intent']\n",
    "    violations = ((~allowed.apply(lambda a: gold.iloc[0] in a)) if False else None)  # placeholder\n",
    "    # compute rowwise violation counts\n",
    "    row_viol = []\n",
    "    for i in range(len(df)):\n",
    "        gi = gold.iloc[i]\n",
    "        ai = allowed.iloc[i]\n",
    "        si = suppressed.iloc[i]\n",
    "        v = 0\n",
    "        if gi not in ai:\n",
    "            v += 1\n",
    "        if gi in si:\n",
    "            v += 1\n",
    "        if len(ai) == 0:\n",
    "            v += 1\n",
    "        row_viol.append(v)\n",
    "    total_violations = sum(1 for v in row_viol if v > 0)\n",
    "else:\n",
    "    total_violations = 0\n",
    "\n",
    "# verdict\n",
    "verdict = 'LEVEL-3 DATASET READY' if total_violations == 0 else 'LEVEL-3 DATASET BLOCKED'\n",
    "\n",
    "# machine-readable summary\n",
    "summary = {\n",
    "    'total_records': total,\n",
    "    'percent_with_hard_constraints': round(pct_hard, 2),\n",
    "    'percent_with_suppressed_intents': round(pct_suppressed, 2),\n",
    "    'logical_violations_count': int(total_violations),\n",
    "    'verdict': verdict\n",
    "}\n",
    "\n",
    "# human-readable summary\n",
    "print('LEVEL-3 Readiness Summary')\n",
    "print('Total records:', total)\n",
    "print(f\"% with hard constraints: {summary['percent_with_hard_constraints']}%\")\n",
    "print(f\"% with suppressed intents: {summary['percent_with_suppressed_intents']}%\")\n",
    "print('Logical violations (rowwise):', summary['logical_violations_count'])\n",
    "print('\\nVERDICT:', summary['verdict'])\n",
    "\n",
    "# expose summary variable for downstream cells\n",
    "LEVEL3_READINESS_SUMMARY = summary\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
