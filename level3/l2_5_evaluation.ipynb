{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Cell-1 — Load Model and Level-3 Dataset\n",
        "import os, sys\n",
        "import pandas as pd\n",
        "from typing import Any, Dict, List\n",
        "import ast\n",
        "\n",
        "# Deterministic repo root discovery\n",
        "def find_repo_root(start_dir=None):\n",
        "    d = start_dir or os.getcwd()\n",
        "    while True:\n",
        "        if os.path.exists(os.path.join(d, 'requirements.txt')) or os.path.exists(os.path.join(d, '.git')):\n",
        "            return d\n",
        "        parent = os.path.dirname(d)\n",
        "        if parent == d:\n",
        "            return os.getcwd()\n",
        "        d = parent\n",
        "repo_root = find_repo_root()\n",
        "sys.path.insert(0, repo_root)\n",
        "\n",
        "# Canonical intents (do not change)\n",
        "INTENTS = ['investigate','execute','summarize','ops']\n",
        "\n",
        "# Attempt to locate a Level-2 inference model deterministically\n",
        "l2_model = None\n",
        "model_name = None\n",
        "candidates = [\n",
        "    ('level2.level2_model','Level2Classifier'),\n",
        "    ('level2.model','Level2Classifier'),\n",
        "    ('level0.level0_model','Level0Classifier')\n",
        "]\n",
        "\n",
        "for mod, cls in candidates:\n",
        "    try:\n",
        "        m = __import__(mod, fromlist=[cls])\n",
        "        ModelClass = getattr(m, cls)\n",
        "        try:\n",
        "            if hasattr(ModelClass, 'load'):\n",
        "                for try_path in [os.path.join(repo_root, 'level2', 'models'), os.path.join(repo_root, 'level0', 'models'), os.path.join(repo_root, mod.split('.')[0], 'models')]:\n",
        "                    if os.path.exists(try_path):\n",
        "                        try:\n",
        "                            l2_model = ModelClass.load(try_path)\n",
        "                            break\n",
        "                        except Exception:\n",
        "                            continue\n",
        "            if l2_model is None:\n",
        "                try:\n",
        "                    l2_model = ModelClass()\n",
        "                except Exception:\n",
        "                    l2_model = None\n",
        "        except Exception:\n",
        "            l2_model = None\n",
        "        if l2_model is not None:\n",
        "            model_name = f'{mod}.{cls}'\n",
        "            break\n",
        "    except Exception:\n",
        "        continue\n",
        "\n",
        "if l2_model is None:\n",
        "    raise RuntimeError('No Level-2 inference model found')\n",
        "\n",
        "# Locate Level-3 dataset\n",
        "candidates_paths = [os.path.join(repo_root, 'l3', 'data', 'level3_intents.csv'), os.path.join(repo_root, 'level3', 'data', 'level3_intents.csv')]\n",
        "level3_path = None\n",
        "for p in candidates_paths:\n",
        "    if os.path.exists(p):\n",
        "        level3_path = p\n",
        "        break\n",
        "if level3_path is None:\n",
        "    raise FileNotFoundError('level3/data/level3_intents.csv not found')\n",
        "\n",
        "l3_df = pd.read_csv(level3_path)\n",
        "required = ['utterance','gold_intent','allowed_intents','suppressed_intents']\n",
        "missing = [c for c in required if c not in l3_df.columns]\n",
        "if missing:\n",
        "    raise ValueError(f'Missing required Level-3 columns: {missing}')\n",
        "\n",
        "def _parse_list_cell(x):\n",
        "    if isinstance(x, (list, tuple)):\n",
        "        return list(x)\n",
        "    if pd.isna(x):\n",
        "        return []\n",
        "    s = str(x).strip()\n",
        "    if s == '':\n",
        "        return []\n",
        "    try:\n",
        "        v = ast.literal_eval(s)\n",
        "        if isinstance(v, (list, tuple)):\n",
        "            return list(v)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return [item.strip() for item in s.split(',') if item.strip()]\n",
        "\n",
        "for c in ['allowed_intents','suppressed_intents']:\n",
        "    l3_df[c] = l3_df[c].apply(_parse_list_cell)\n",
        "\n",
        "L2_MODEL = l2_model\n",
        "MODEL_NAME = model_name\n",
        "L3_DF = l3_df\n",
        "INTENT_ORDER = INTENTS\n",
        "print(f'Loaded model: {MODEL_NAME}')\n",
        "print(f'Loaded {len(L3_DF)} records from Level-3 dataset')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Cell-2 — Baseline Level-2 Inference (Unconstrained)\n",
        "from typing import Dict\n",
        "\n",
        "def predict_probs(utterance: str) -> Dict[str, float]:\n",
        "    try:\n",
        "        if hasattr(L2_MODEL, 'predict_proba') and hasattr(L2_MODEL, 'classes_'):\n",
        "            probs = L2_MODEL.predict_proba([utterance])[0]\n",
        "            classes = list(L2_MODEL.classes_)\n",
        "            mapping = {c: float(probs[i]) for i,c in enumerate(classes)}\n",
        "            return {k: float(mapping.get(k, 0.0)) for k in INTENT_ORDER}\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        out = L2_MODEL.predict(utterance)\n",
        "        if isinstance(out, dict) and 'probabilities' in out:\n",
        "            probs = out['probabilities']\n",
        "            return {k: float(probs.get(k, 0.0)) for k in INTENT_ORDER}\n",
        "        if isinstance(out, dict) and 'intent' in out and 'confidence' in out:\n",
        "            p = {k: 0.0 for k in INTENT_ORDER}\n",
        "            if out['intent'] in p:\n",
        "                p[out['intent']] = float(out['confidence'])\n",
        "            return p\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        out = L2_MODEL.predict_proba(utterance)\n",
        "        if isinstance(out, dict):\n",
        "            return {k: float(out.get(k, 0.0)) for k in INTENT_ORDER}\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        lab = None\n",
        "        pred = L2_MODEL.predict(utterance)\n",
        "        if isinstance(pred, dict) and 'intent' in pred:\n",
        "            lab = pred['intent']\n",
        "        elif isinstance(pred, str):\n",
        "            lab = pred\n",
        "        p = {k: 0.0 for k in INTENT_ORDER}\n",
        "        if lab in p:\n",
        "            p[lab] = 1.0\n",
        "        return p\n",
        "    except Exception:\n",
        "        raise RuntimeError('Unable to obtain model predictions')\n",
        "\n",
        "rows = []\n",
        "for _, r in L3_DF.iterrows():\n",
        "    utt = r['utterance']\n",
        "    probs = predict_probs(utt)\n",
        "    s = sum(probs.values())\n",
        "    if s <= 0:\n",
        "        probs = {k: 1.0/len(INTENT_ORDER) for k in INTENT_ORDER}\n",
        "        s = 1.0\n",
        "    if abs(s - 1.0) > 1e-12:\n",
        "        probs = {k: float(v)/float(s) for k,v in probs.items()}\n",
        "    top_intent = max(probs.items(), key=lambda iv: (iv[1], iv[0]))[0]\n",
        "    top_score = probs[top_intent]\n",
        "    rows.append({\n",
        "        'utterance': utt,\n",
        "        'gold_intent': r['gold_intent'],\n",
        "        'raw_probs': probs,\n",
        "        'raw_top_intent': top_intent,\n",
        "        'raw_top_score': float(top_score),\n",
        "        'allowed_intents': r['allowed_intents'],\n",
        "        'suppressed_intents': r['suppressed_intents']\n",
        "    })\n",
        "BASELINE_DF = pd.DataFrame(rows)\n",
        "L2_BASELINE = BASELINE_DF\n",
        "print(f'Completed baseline inference on {len(L2_BASELINE)} utterances')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Cell-3 — Apply Level-2.5 Logical Constraints (Inference-Time Only)\n",
        "from typing import Any, Dict\n",
        "\n",
        "def apply_constraints_row(row) -> Dict[str, Any]:\n",
        "    probs = dict(row['raw_probs'])\n",
        "    allowed = [a.strip().lower() for a in (row.get('allowed_intents') or [])]\n",
        "    suppressed = [s.strip().lower() for s in (row.get('suppressed_intents') or [])]\n",
        "    for s in suppressed:\n",
        "        if s in probs:\n",
        "            probs[s] = 0.0\n",
        "    if len(allowed) > 0:\n",
        "        for k in list(probs.keys()):\n",
        "            if k not in allowed:\n",
        "                probs[k] = 0.0\n",
        "    total = sum(probs.values())\n",
        "    constrained_flag = True\n",
        "    if total <= 0.0:\n",
        "        probs = dict(row['raw_probs'])\n",
        "        constrained_flag = False\n",
        "        total = sum(probs.values())\n",
        "    if abs(total - 1.0) > 1e-12:\n",
        "        probs = {k: float(v)/float(total) for k,v in probs.items()}\n",
        "    top_intent = max(probs.items(), key=lambda iv: (iv[1], iv[0]))[0]\n",
        "    top_score = probs[top_intent]\n",
        "    return {\n",
        "        'constrained_probs': probs,\n",
        "        'constrained_top_intent': top_intent,\n",
        "        'constrained_top_score': float(top_score),\n",
        "        'constrained_applied': constrained_flag\n",
        "    }\n",
        "\n",
        "rows = []\n",
        "for _, r in L2_BASELINE.iterrows():\n",
        "    rows.append(apply_constraints_row(r))\n",
        "CONSTRAINED_DF = pd.concat([L2_BASELINE.reset_index(drop=True), pd.DataFrame(rows)], axis=1)\n",
        "L2_2P5 = CONSTRAINED_DF\n",
        "print(f'Applied Level-2.5 constraints to {len(L2_2P5)} predictions')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Cell-4 — Comparative Metrics (Before vs After)\n",
        "def is_violating_raw(row):\n",
        "    ai = [a.strip().lower() for a in (row.get('allowed_intents') or [])]\n",
        "    si = [s.strip().lower() for s in (row.get('suppressed_intents') or [])]\n",
        "    rt = str(row['raw_top_intent']).strip().lower()\n",
        "    if rt in si:\n",
        "        return True\n",
        "    if len(ai) > 0 and rt not in ai:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def is_violating_constrained(row):\n",
        "    ai = [a.strip().lower() for a in (row.get('allowed_intents') or [])]\n",
        "    si = [s.strip().lower() for s in (row.get('suppressed_intents') or [])]\n",
        "    ct = str(row['constrained_top_intent']).strip().lower()\n",
        "    if ct in si:\n",
        "        return True\n",
        "    if len(ai) > 0 and ct not in ai:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "df = L2_2P5.copy()\n",
        "df['viol_raw'] = df.apply(is_violating_raw, axis=1)\n",
        "df['viol_constrained'] = df.apply(is_violating_constrained, axis=1)\n",
        "total = len(df)\n",
        "viol_before = int(df['viol_raw'].sum())\n",
        "viol_after = int(df['viol_constrained'].sum())\n",
        "pct_before = viol_before/total*100 if total else 0.0\n",
        "pct_after = viol_after/total*100 if total else 0.0\n",
        "flips = (df['raw_top_intent'] != df['constrained_top_intent']).sum()\n",
        "flip_rate = int(flips)/total*100 if total else 0.0\n",
        "df['delta_top_score'] = df['constrained_top_score'] - df['raw_top_score']\n",
        "mean_delta = float(df['delta_top_score'].mean())\n",
        "dist_before = df['raw_top_intent'].value_counts().to_dict()\n",
        "dist_after = df['constrained_top_intent'].value_counts().to_dict()\n",
        "print('Comparative Metrics (Before vs After)')\n",
        "print(f'Total records: {total}')\n",
        "print(f'% violating before: {pct_before:.2f}% ({viol_before})')\n",
        "print(f'% violating after : {pct_after:.2f}% ({viol_after})')\n",
        "print(f'Intent flip rate: {flip_rate:.2f}% ({flips})')\n",
        "print(f'Mean change in top-score (after - before): {mean_delta:.4f}')\n",
        "print('\\nDistribution before (intent: count)')\n",
        "for k in INTENTS:\n",
        "    print(f'  {k}: {int(dist_before.get(k,0))}')\n",
        "print('\\nDistribution after (intent: count)')\n",
        "for k in INTENTS:\n",
        "    print(f'  {k}: {int(dist_after.get(k,0))}')\n",
        "\n",
        "# Save violation columns back to L2_2P5 for downstream cells\n",
        "L2_2P5 = df\n",
        "\n",
        "L25_METRICS = {\n",
        "    'total': total,\n",
        "    'viol_before': viol_before,\n",
        "    'viol_after': viol_after,\n",
        "    'pct_before': pct_before,\n",
        "    'pct_after': pct_after,\n",
        "    'flip_count': int(flips),\n",
        "    'flip_rate_pct': flip_rate,\n",
        "    'mean_delta_top_score': mean_delta,\n",
        "    'dist_before': dist_before,\n",
        "    'dist_after': dist_after\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Cell-5 — Concrete Example Review\n",
        "candidates = L2_2P5[L2_2P5['viol_raw'] == True].copy()\n",
        "corrected = candidates[(candidates['viol_constrained'] == False) & (candidates['raw_top_intent'] != candidates['constrained_top_intent'])]\n",
        "corrected = corrected.sort_values(by='utterance')\n",
        "sample = corrected.head(10)\n",
        "print('Concrete examples where L2 was invalid and L2.5 corrected:')\n",
        "for _, r in sample.iterrows():\n",
        "    print('\\n---')\n",
        "    print('utterance :', r['utterance'])\n",
        "    print('raw L2   :', r['raw_top_intent'])\n",
        "    print('constrained L2.5 :', r['constrained_top_intent'])\n",
        "    print('suppressed intents :', r['suppressed_intents'])\n",
        "\n",
        "L25_EXAMPLES = sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Cell-6 — Level-2.5 Verdict\n",
        "before = L25_METRICS['viol_before']\n",
        "after = L25_METRICS['viol_after']\n",
        "reduction = before - after\n",
        "reduction_pct = (reduction / before) if before else 0.0\n",
        "if reduction_pct > 0.10:\n",
        "    verdict = 'LEVEL-2.5 SHOWS CLEAR VALUE'\n",
        "else:\n",
        "    verdict = 'LEVEL-2.5 SHOWS LIMITED VALUE'\n",
        "if reduction_pct > 0.10:\n",
        "    justification = 'Constraint application reduced logical violations by more than 10% — Level-3 modeling may be justified.'\n",
        "else:\n",
        "    justification = 'Constraint application produced limited reduction in violations — further Level-3 modeling should be evaluated conservatively.'\n",
        "print(verdict)\n",
        "print(f'Violations before: {before}, after: {after}, reduction: {reduction} ({reduction_pct:.2%})')\n",
        "print(justification)\n",
        "L25_VERDICT = {'verdict': verdict, 'before': before, 'after': after, 'reduction': reduction, 'reduction_pct': reduction_pct}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# What did Level-2.5 prove?\n",
        "\n",
        "## 1. What kinds of errors Level-2 makes\n",
        "\n",
        "Level-2 models (TF-IDF, embeddings, or neural classifiers) produce predictions based purely on statistical patterns in training data. This structural design creates characteristic error modes:\n",
        "\n",
        "- **Logical contradictions**: The model can assign high probability to mutually exclusive intents (e.g., predicting both `execute` and `investigate` for the same utterance, when investigation should precede action).\n",
        "\n",
        "- **Overconfident but invalid predictions**: When an utterance matches training examples strongly, the model confidently predicts intents that violate domain rules (e.g., executing an action that the user's role forbids).\n",
        "\n",
        "- **Ambiguous cases where multiple intents compete**: Without explicit logic, the model has no principled way to resolve conflicts between valid but contradictory interpretations.\n",
        "\n",
        "- **Errors caused by lack of structural constraints**: The model learns correlations but not causality. It cannot distinguish \"utterances that happened to co-occur with intent X\" from \"utterances that must produce intent X given the domain logic.\"\n",
        "\n",
        "**This is not a data quality problem or a model weakness problem.** These errors arise because statistical models fundamentally lack mechanisms to enforce logical constraints. The model architecture itself has no space to represent rules like \"if suppressed, probability must be zero.\"\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Which errors are logically impossible vs. undesirable\n",
        "\n",
        "Not all errors are equal. NSAI systems distinguish:\n",
        "\n",
        "### Logically impossible errors\n",
        "These violate explicit domain rules and should **never** occur:\n",
        "\n",
        "- **Violations of explicit domain rules**: Predicting intents that are explicitly suppressed for a given utterance\n",
        "- **Mutually exclusive intent combinations**: Assigning probability to intents that cannot co-exist by definition\n",
        "- **Actions that should never occur given the utterance form**: For example, executing a command when the utterance is phrased as a question\n",
        "\n",
        "These errors are **hard constraints** — they represent logical contradictions, not just poor choices.\n",
        "\n",
        "### Logically undesirable errors\n",
        "These are valid under the rules but suboptimal:\n",
        "\n",
        "- **Valid but suboptimal intent selection**: Choosing `summarize` when `investigate` would yield better outcomes\n",
        "- **Poor prioritization under uncertainty**: Assigning equal weight to all allowed intents when context suggests one is more appropriate\n",
        "- **Premature action vs. investigation**: Immediately executing when gathering information first would be safer\n",
        "\n",
        "These errors represent **soft preferences** — the model made a legal move, but not the best one.\n",
        "\n",
        "**This distinction is the foundation of neuro-symbolic AI.** Hard constraints belong in logic. Soft preferences belong in learned models.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. How Level-2.5 fixes these without retraining\n",
        "\n",
        "Level-2.5 applies logical constraints **after** the Level-2 model produces its predictions:\n",
        "\n",
        "1. **The model predicts freely**: Level-2 generates a full probability distribution over all intents, using only statistical patterns.\n",
        "\n",
        "2. **Logic filters invalid outputs**: For each prediction, Level-2.5 sets the probability of suppressed intents to zero. If `allowed_intents` is specified, it zeros all intents not in that list.\n",
        "\n",
        "3. **Remaining probabilities are re-normalized**: After removing invalid options, the remaining probabilities are scaled to sum to 1.0.\n",
        "\n",
        "4. **The model itself is unchanged**: No weights are updated. No retraining occurs. The model's internal representations remain identical.\n",
        "\n",
        "**The model still thinks freely — logic only corrects the output.** The Level-2 model may internally prefer invalid paths, but those preferences are masked before the user sees them. This is efficient and interpretable, but it comes with fundamental limitations.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. What Level-2.5 cannot fix\n",
        "\n",
        "Level-2.5 proves that post-hoc logic helps, but it also reveals why post-hoc logic is not enough:\n",
        "\n",
        "- **The model can still internally prefer invalid paths**: Even though we zero out suppressed intents, the model's embeddings and learned weights still encode those invalid patterns. The model wastes representational capacity on options it will never select.\n",
        "\n",
        "- **Conflicts are resolved late**: By the time logic intervenes, the model has already committed to a prediction. If the top-ranked intent is invalid, we fall back to the second choice — but that second choice was learned without knowledge that the first was impossible.\n",
        "\n",
        "- **Representation learning is unchanged**: The model's hidden layers continue to encode features that correlate with suppressed intents. These features pollute the representation space and interfere with learning valid patterns.\n",
        "\n",
        "- **Logic cannot shape how the model reasons**: Constraints that apply during inference cannot influence how the model learns to decompose the problem. If certain intent combinations are impossible, the model should learn features that reflect that structure — but Level-2.5 cannot teach it to do so.\n",
        "\n",
        "**This is why Level-3 exists.** To fix these limitations, logic must be embedded **inside** the model architecture, not applied as a post-processing filter.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Key takeaway\n",
        "\n",
        "**Level-2.5 proves that logical constraints reduce invalid predictions.** In cases where the base model frequently violates domain rules, inference-time filtering can deliver immediate value without retraining.\n",
        "\n",
        "**Level-2.5 also proves that post-hoc logic has fundamental limits.** The model learns and reasons without awareness of constraints, leading to wasted capacity and suboptimal representations.\n",
        "\n",
        "**True neuro-symbolic reasoning requires logic inside the model** — not as a filter, but as a structural component of learning and inference. That is the transition from Level-2.5 to Level-3."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}