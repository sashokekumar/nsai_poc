{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8946cda3",
   "metadata": {},
   "source": [
    "# Level 0 vs Level 1: Model Comparison & Validation\n",
    "\n",
    "**Purpose**: Compare baseline statistical classifier (Level 0) with neuro-symbolic approach (Level 1)\n",
    "\n",
    "**Goals**:\n",
    "- Send identical inputs to both models\n",
    "- Compare predictions and confidence scores\n",
    "- Identify ambiguity differences\n",
    "- Highlight cases where rules change the decision\n",
    "- Validate Level 1 improvements over Level 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4174d22",
   "metadata": {},
   "source": [
    "## Setup: Load Both Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492e25d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f3bdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load shared dataset\n",
    "df = pd.read_csv('../data/intents_base.csv')\n",
    "df['intent'] = df['intent'].str.lower().str.strip()\n",
    "\n",
    "X = df['utterance']\n",
    "y = df['intent']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Dataset loaded: {len(df)} records\")\n",
    "print(f\"Train: {len(X_train)} | Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181784ff",
   "metadata": {},
   "source": [
    "## Level 0: Baseline Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c868b2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Level 0 Configuration\n",
    "LEVEL0_CONFIDENCE_THRESHOLD = 0.7\n",
    "\n",
    "# Train Level 0 model\n",
    "level0_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        max_features=5000,\n",
    "        ngram_range=(1, 2),\n",
    "        stop_words='english'\n",
    "    )),\n",
    "    ('classifier', LogisticRegression(\n",
    "        solver='lbfgs',\n",
    "        random_state=42,\n",
    "        max_iter=1000\n",
    "    ))\n",
    "])\n",
    "\n",
    "level0_pipeline.fit(X_train, y_train)\n",
    "print(\"Level 0 model trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d187f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Level 0 Prediction Function\n",
    "def level0_predict(text):\n",
    "    \"\"\"Level 0: TF-IDF + LR with confidence threshold\"\"\"\n",
    "    pred_intent = level0_pipeline.predict([text])[0]\n",
    "    proba = level0_pipeline.predict_proba([text])[0]\n",
    "    confidence = float(np.max(proba))\n",
    "    \n",
    "    # Get probabilities for all classes\n",
    "    classes = level0_pipeline.named_steps['classifier'].classes_\n",
    "    proba_dict = {intent: float(proba[i]) for i, intent in enumerate(classes)}\n",
    "    \n",
    "    # Confidence thresholding\n",
    "    final_intent = pred_intent if confidence >= LEVEL0_CONFIDENCE_THRESHOLD else 'abstain'\n",
    "    \n",
    "    return {\n",
    "        'predicted_intent': pred_intent,\n",
    "        'final_intent': final_intent,\n",
    "        'confidence': confidence,\n",
    "        'abstain': confidence < LEVEL0_CONFIDENCE_THRESHOLD,\n",
    "        'probabilities': proba_dict\n",
    "    }\n",
    "\n",
    "print(\"Level 0 prediction function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3b09ac",
   "metadata": {},
   "source": [
    "## Level 1: Neuro-Symbolic Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eea1102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Level 1 Configuration\n",
    "CONFIG = {\n",
    "    'BASE_MIN_CONF': 0.60,\n",
    "    'MIN_MARGIN': 0.10,\n",
    "    'EXECUTION_MIN_CONF': 0.85,\n",
    "    'MIN_TOKENS_OUT_OF_SCOPE': 3,\n",
    "}\n",
    "\n",
    "RULE_PRIORITY = [\"R1\", \"R4\", \"R2\", \"R3\"]\n",
    "\n",
    "# Train Level 1 model (same statistical base as Level 0)\n",
    "level1_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        max_features=5000,\n",
    "        ngram_range=(1, 2),\n",
    "        stop_words='english'\n",
    "    )),\n",
    "    ('classifier', LogisticRegression(\n",
    "        solver='lbfgs',\n",
    "        random_state=42,\n",
    "        max_iter=1000\n",
    "    ))\n",
    "])\n",
    "\n",
    "level1_pipeline.fit(X_train, y_train)\n",
    "print(\"Level 1 model trained\")\n",
    "print(f\"Rule Priority: {RULE_PRIORITY}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ac149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Level 1 Signal Extraction\n",
    "def extract_signals(text):\n",
    "    \"\"\"Extract signals from Level 1 model\"\"\"\n",
    "    proba = level1_pipeline.predict_proba([text])[0]\n",
    "    classes = level1_pipeline.named_steps['classifier'].classes_\n",
    "    \n",
    "    sorted_indices = np.argsort(proba)[::-1]\n",
    "    max_confidence = float(proba[sorted_indices[0]])\n",
    "    second_best_confidence = float(proba[sorted_indices[1]])\n",
    "    margin = max_confidence - second_best_confidence\n",
    "    \n",
    "    # Get TF-IDF tokens\n",
    "    tfidf_vec = level1_pipeline.named_steps['tfidf'].transform([text])\n",
    "    active_features = tfidf_vec.toarray()[0]\n",
    "    meaningful_tokens = np.sum(active_features > 0)\n",
    "    \n",
    "    return {\n",
    "        'probabilities': {intent: float(proba[i]) for i, intent in enumerate(classes)},\n",
    "        'max_confidence': max_confidence,\n",
    "        'second_best_confidence': second_best_confidence,\n",
    "        'margin': margin,\n",
    "        'predicted_intent': classes[sorted_indices[0]],\n",
    "        'meaningful_tokens': int(meaningful_tokens)\n",
    "    }\n",
    "\n",
    "print(\"Level 1 signal extraction ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00186852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Level 1 Decision Rules\n",
    "def apply_decision_rules(signals):\n",
    "    \"\"\"Apply Level 1 neuro-symbolic rules\"\"\"\n",
    "    triggered_rules = []\n",
    "    predicted_intent = signals['predicted_intent']\n",
    "    decision_state = 'accepted'\n",
    "    decision_reason = 'model_prediction'\n",
    "    \n",
    "    # R1: Quality Gate\n",
    "    if signals['meaningful_tokens'] < CONFIG['MIN_TOKENS_OUT_OF_SCOPE']:\n",
    "        triggered_rules.append({\n",
    "            'rule_id': 'R1',\n",
    "            'category': 'quality',\n",
    "            'priority': 100,\n",
    "            'condition': f\"meaningful_tokens < {CONFIG['MIN_TOKENS_OUT_OF_SCOPE']}\",\n",
    "            'signal_value': signals['meaningful_tokens']\n",
    "        })\n",
    "        decision_state = 'blocked'\n",
    "        decision_reason = 'insufficient_tokens'\n",
    "        return {\n",
    "            'triggered_rules': triggered_rules,\n",
    "            'predicted_intent': predicted_intent,\n",
    "            'decision_state': decision_state,\n",
    "            'decision_reason': decision_reason\n",
    "        }\n",
    "    \n",
    "    # R4: Safety Gate\n",
    "    if signals['predicted_intent'] == 'execution' and signals['max_confidence'] < CONFIG['EXECUTION_MIN_CONF']:\n",
    "        triggered_rules.append({\n",
    "            'rule_id': 'R4',\n",
    "            'category': 'safety',\n",
    "            'priority': 90,\n",
    "            'condition': f\"predicted_intent==execution AND max_confidence < {CONFIG['EXECUTION_MIN_CONF']}\",\n",
    "            'signal_value': signals['max_confidence']\n",
    "        })\n",
    "        if signals['max_confidence'] >= CONFIG['BASE_MIN_CONF']:\n",
    "            decision_state = 'blocked'\n",
    "            decision_reason = 'execution_safety_block'\n",
    "        else:\n",
    "            decision_state = 'needs_clarification'\n",
    "            decision_reason = 'execution_low_confidence'\n",
    "        return {\n",
    "            'triggered_rules': triggered_rules,\n",
    "            'predicted_intent': predicted_intent,\n",
    "            'decision_state': decision_state,\n",
    "            'decision_reason': decision_reason\n",
    "        }\n",
    "    \n",
    "    # R2 & R3: Ambiguity Gates\n",
    "    ambiguity_detected = False\n",
    "    \n",
    "    if signals['max_confidence'] < CONFIG['BASE_MIN_CONF']:\n",
    "        triggered_rules.append({\n",
    "            'rule_id': 'R2',\n",
    "            'category': 'ambiguity',\n",
    "            'priority': 50,\n",
    "            'condition': f\"max_confidence < {CONFIG['BASE_MIN_CONF']}\",\n",
    "            'signal_value': signals['max_confidence']\n",
    "        })\n",
    "        ambiguity_detected = True\n",
    "    \n",
    "    if signals['margin'] < CONFIG['MIN_MARGIN']:\n",
    "        triggered_rules.append({\n",
    "            'rule_id': 'R3',\n",
    "            'category': 'ambiguity',\n",
    "            'priority': 40,\n",
    "            'condition': f\"margin < {CONFIG['MIN_MARGIN']}\",\n",
    "            'signal_value': signals['margin']\n",
    "        })\n",
    "        ambiguity_detected = True\n",
    "    \n",
    "    if ambiguity_detected:\n",
    "        decision_state = 'needs_clarification'\n",
    "        decision_reason = 'ambiguous_prediction'\n",
    "    \n",
    "    return {\n",
    "        'triggered_rules': triggered_rules,\n",
    "        'predicted_intent': predicted_intent,\n",
    "        'decision_state': decision_state,\n",
    "        'decision_reason': decision_reason\n",
    "    }\n",
    "\n",
    "print(\"Level 1 decision rules ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a315043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Level 1 Prediction Function\n",
    "def level1_predict(text):\n",
    "    \"\"\"Level 1: Statistical + Neuro-Symbolic Rules\"\"\"\n",
    "    signals = extract_signals(text)\n",
    "    rules_output = apply_decision_rules(signals)\n",
    "    \n",
    "    return {\n",
    "        'predicted_intent': rules_output['predicted_intent'],\n",
    "        'decision_state': rules_output['decision_state'],\n",
    "        'decision_reason': rules_output['decision_reason'],\n",
    "        'confidence': signals['max_confidence'],\n",
    "        'margin': signals['margin'],\n",
    "        'meaningful_tokens': signals['meaningful_tokens'],\n",
    "        'triggered_rules': rules_output['triggered_rules'],\n",
    "        'probabilities': signals['probabilities']\n",
    "    }\n",
    "\n",
    "print(\"Level 1 prediction function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0b0300",
   "metadata": {},
   "source": [
    "## Comparison Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b147fd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_predictions(utterance):\n",
    "    \"\"\"Compare Level 0 and Level 1 predictions side-by-side\"\"\"\n",
    "    level0_result = level0_predict(utterance)\n",
    "    level1_result = level1_predict(utterance)\n",
    "    \n",
    "    return {\n",
    "        'utterance': utterance,\n",
    "        'level0': level0_result,\n",
    "        'level1': level1_result\n",
    "    }\n",
    "\n",
    "def display_comparison(comparison):\n",
    "    \"\"\"Display comparison in readable format\"\"\"\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"UTTERANCE: {comparison['utterance']}\")\n",
    "    print('='*100)\n",
    "    \n",
    "    l0 = comparison['level0']\n",
    "    l1 = comparison['level1']\n",
    "    \n",
    "    # Side-by-side comparison\n",
    "    print(f\"\\n{'LEVEL 0 (Baseline)':<50} | {'LEVEL 1 (Neuro-Symbolic)'}\")\n",
    "    print('-'*100)\n",
    "    print(f\"{'Predicted Intent: ' + l0['predicted_intent']:<50} | {'Predicted Intent: ' + l1['predicted_intent']}\")\n",
    "    print(f\"{'Final Decision: ' + l0['final_intent']:<50} | {'Decision State: ' + l1['decision_state']}\")\n",
    "    print(f\"{'Confidence: ' + f\"{l0['confidence']:.4f}\":<50} | {'Confidence: ' + f\"{l1['confidence']:.4f}\"}\")\n",
    "    print(f\"{'Abstain: ' + str(l0['abstain']):<50} | {'Margin: ' + f\"{l1['margin']:.4f}\"}\")\n",
    "    print(f\"{'â€”':<50} | {'Tokens: ' + str(l1['meaningful_tokens'])}\")\n",
    "    \n",
    "    # Level 1 Rules\n",
    "    if l1['triggered_rules']:\n",
    "        print(f\"{'â€”':<50} | {'Triggered Rules: ' + ', '.join([r['rule_id'] for r in l1['triggered_rules']])}\")\n",
    "        print(f\"{'â€”':<50} | {'Decision Reason: ' + l1['decision_reason']}\")\n",
    "    else:\n",
    "        print(f\"{'â€”':<50} | {'Triggered Rules: None'}\")\n",
    "    \n",
    "    # Ambiguity Detection\n",
    "    print(f\"\\n{'-'*100}\")\n",
    "    print(\"AMBIGUITY ANALYSIS:\")\n",
    "    \n",
    "    # Check for disagreement\n",
    "    intent_match = l0['predicted_intent'] == l1['predicted_intent']\n",
    "    print(f\"  Intent Agreement: {'âœ“ YES' if intent_match else 'âœ— NO (DISAGREEMENT)'}\")\n",
    "    \n",
    "    # Level 0 ambiguity (abstain)\n",
    "    if l0['abstain']:\n",
    "        print(f\"  Level 0 Ambiguity: âš  ABSTAIN (confidence {l0['confidence']:.4f} < 0.70)\")\n",
    "    else:\n",
    "        print(f\"  Level 0 Ambiguity: âœ“ Confident\")\n",
    "    \n",
    "    # Level 1 ambiguity (rules)\n",
    "    if l1['decision_state'] == 'needs_clarification':\n",
    "        print(f\"  Level 1 Ambiguity: âš  NEEDS CLARIFICATION ({l1['decision_reason']})\")\n",
    "    elif l1['decision_state'] == 'blocked':\n",
    "        print(f\"  Level 1 Ambiguity: ðŸš« BLOCKED ({l1['decision_reason']})\")\n",
    "    else:\n",
    "        print(f\"  Level 1 Ambiguity: âœ“ Accepted\")\n",
    "    \n",
    "    # Probability distribution\n",
    "    print(f\"\\n{'-'*100}\")\n",
    "    print(\"PROBABILITY DISTRIBUTION:\")\n",
    "    for intent in sorted(l0['probabilities'].keys()):\n",
    "        l0_prob = l0['probabilities'][intent]\n",
    "        l1_prob = l1['probabilities'][intent]\n",
    "        diff = abs(l0_prob - l1_prob)\n",
    "        match = \"âœ“\" if diff < 0.001 else \"âœ—\"\n",
    "        print(f\"  {intent:<20} L0: {l0_prob:.4f}  |  L1: {l1_prob:.4f}  {match}\")\n",
    "    \n",
    "    print('='*100)\n",
    "\n",
    "print(\"Comparison functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1191dde",
   "metadata": {},
   "source": [
    "## Test Cases: Curated Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b494c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test utterances covering different scenarios\n",
    "test_utterances = [\n",
    "    # High confidence cases\n",
    "    \"why is server cpu high\",\n",
    "    \"summarize the incident from yesterday\",\n",
    "    \n",
    "    # Execution cases (should trigger R4 in Level 1)\n",
    "    \"restart nginx on host123\",\n",
    "    \"delete all logs from production\",\n",
    "    \n",
    "    # Low token cases (should trigger R1 in Level 1)\n",
    "    \"hello\",\n",
    "    \"hi\",\n",
    "    \"ok\",\n",
    "    \n",
    "    # Ambiguous cases\n",
    "    \"server issues\",\n",
    "    \"check status\",\n",
    "    \"what happened\",\n",
    "    \n",
    "    # Edge cases\n",
    "    \"restart the server please\",\n",
    "    \"tell me about server problems\",\n",
    "    \"can you summarize\",\n",
    "]\n",
    "\n",
    "print(f\"Prepared {len(test_utterances)} test utterances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea789a4",
   "metadata": {},
   "source": [
    "## Run Comparison: All Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61695acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all test cases\n",
    "comparisons = []\n",
    "\n",
    "for utterance in test_utterances:\n",
    "    comparison = compare_predictions(utterance)\n",
    "    comparisons.append(comparison)\n",
    "    display_comparison(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12ed25c",
   "metadata": {},
   "source": [
    "## Summary: Disagreements & Ambiguity Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cf03b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze all comparisons\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"SUMMARY: Level 0 vs Level 1\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "disagreements = []\n",
    "level0_abstains = []\n",
    "level1_blocks = []\n",
    "level1_clarifications = []\n",
    "rule_triggers = {'R1': 0, 'R2': 0, 'R3': 0, 'R4': 0}\n",
    "\n",
    "for comp in comparisons:\n",
    "    l0 = comp['level0']\n",
    "    l1 = comp['level1']\n",
    "    utterance = comp['utterance']\n",
    "    \n",
    "    # Track disagreements\n",
    "    if l0['predicted_intent'] != l1['predicted_intent']:\n",
    "        disagreements.append({\n",
    "            'utterance': utterance,\n",
    "            'level0': l0['predicted_intent'],\n",
    "            'level1': l1['predicted_intent']\n",
    "        })\n",
    "    \n",
    "    # Track Level 0 abstains\n",
    "    if l0['abstain']:\n",
    "        level0_abstains.append(utterance)\n",
    "    \n",
    "    # Track Level 1 blocks\n",
    "    if l1['decision_state'] == 'blocked':\n",
    "        level1_blocks.append({\n",
    "            'utterance': utterance,\n",
    "            'reason': l1['decision_reason']\n",
    "        })\n",
    "    \n",
    "    # Track Level 1 clarifications\n",
    "    if l1['decision_state'] == 'needs_clarification':\n",
    "        level1_clarifications.append({\n",
    "            'utterance': utterance,\n",
    "            'reason': l1['decision_reason']\n",
    "        })\n",
    "    \n",
    "    # Track rule triggers\n",
    "    for rule in l1['triggered_rules']:\n",
    "        rule_triggers[rule['rule_id']] += 1\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\nTotal Test Cases: {len(comparisons)}\")\n",
    "print(f\"\\nIntent Disagreements: {len(disagreements)}\")\n",
    "if disagreements:\n",
    "    for d in disagreements:\n",
    "        print(f\"  â€¢ \\\"{d['utterance']}\\\" â†’ L0: {d['level0']}, L1: {d['level1']}\")\n",
    "\n",
    "print(f\"\\nLevel 0 Abstains: {len(level0_abstains)}\")\n",
    "if level0_abstains:\n",
    "    for utterance in level0_abstains:\n",
    "        print(f\"  â€¢ \\\"{utterance}\\\"\")\n",
    "\n",
    "print(f\"\\nLevel 1 Blocked: {len(level1_blocks)}\")\n",
    "if level1_blocks:\n",
    "    for item in level1_blocks:\n",
    "        print(f\"  â€¢ \\\"{item['utterance']}\\\" â†’ {item['reason']}\")\n",
    "\n",
    "print(f\"\\nLevel 1 Needs Clarification: {len(level1_clarifications)}\")\n",
    "if level1_clarifications:\n",
    "    for item in level1_clarifications:\n",
    "        print(f\"  â€¢ \\\"{item['utterance']}\\\" â†’ {item['reason']}\")\n",
    "\n",
    "print(f\"\\nLevel 1 Rule Triggers:\")\n",
    "for rule_id in RULE_PRIORITY:\n",
    "    count = rule_triggers[rule_id]\n",
    "    if count > 0:\n",
    "        print(f\"  {rule_id}: {count} times\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40dfd6e",
   "metadata": {},
   "source": [
    "## Key Findings & Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd472495",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"KEY INSIGHTS:\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(\"\\n1. MODEL AGREEMENT:\")\n",
    "agreement_rate = (len(comparisons) - len(disagreements)) / len(comparisons) * 100\n",
    "print(f\"   - Intent agreement rate: {agreement_rate:.1f}%\")\n",
    "print(f\"   - Both models use same statistical base (TF-IDF + LR)\")\n",
    "print(f\"   - Disagreements indicate potential model instability\")\n",
    "\n",
    "print(\"\\n2. AMBIGUITY HANDLING:\")\n",
    "print(f\"   - Level 0: Uses fixed threshold (0.7), {len(level0_abstains)} abstains\")\n",
    "print(f\"   - Level 1: Uses rules + thresholds, {len(level1_clarifications)} clarifications\")\n",
    "print(f\"   - Level 1 adds execution safety: {len(level1_blocks)} blocked\")\n",
    "\n",
    "print(\"\\n3. RULE EFFECTIVENESS:\")\n",
    "total_rule_triggers = sum(rule_triggers.values())\n",
    "print(f\"   - Total rule activations: {total_rule_triggers}\")\n",
    "print(f\"   - Rules changed decision in {len(level1_blocks) + len(level1_clarifications)} cases\")\n",
    "print(f\"   - Safety gates (R4) prevented risky executions: {rule_triggers['R4']} times\")\n",
    "\n",
    "print(\"\\n4. LEVEL 1 IMPROVEMENTS:\")\n",
    "print(\"   âœ“ Separates intent (what user wants) from decision (what system does)\")\n",
    "print(\"   âœ“ Adds execution safety layer\")\n",
    "print(\"   âœ“ Detects low-quality inputs (insufficient tokens)\")\n",
    "print(\"   âœ“ Uses margin in addition to confidence\")\n",
    "print(\"   âœ“ Provides structured, explainable decisions\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcf78fb",
   "metadata": {},
   "source": [
    "## Export Results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c76362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_records = []\n",
    "\n",
    "for comp in comparisons:\n",
    "    l0 = comp['level0']\n",
    "    l1 = comp['level1']\n",
    "    \n",
    "    comparison_records.append({\n",
    "        'utterance': comp['utterance'],\n",
    "        'l0_predicted_intent': l0['predicted_intent'],\n",
    "        'l0_final_intent': l0['final_intent'],\n",
    "        'l0_confidence': l0['confidence'],\n",
    "        'l0_abstain': l0['abstain'],\n",
    "        'l1_predicted_intent': l1['predicted_intent'],\n",
    "        'l1_decision_state': l1['decision_state'],\n",
    "        'l1_decision_reason': l1['decision_reason'],\n",
    "        'l1_confidence': l1['confidence'],\n",
    "        'l1_margin': l1['margin'],\n",
    "        'l1_tokens': l1['meaningful_tokens'],\n",
    "        'l1_rules_triggered': ', '.join([r['rule_id'] for r in l1['triggered_rules']]) if l1['triggered_rules'] else 'None',\n",
    "        'intent_agreement': l0['predicted_intent'] == l1['predicted_intent']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_records)\n",
    "\n",
    "# Save to CSV\n",
    "output_file = 'validation_results.csv'\n",
    "comparison_df.to_csv(output_file, index=False)\n",
    "print(f\"Comparison results saved to: {output_file}\")\n",
    "print(f\"\\nPreview:\")\n",
    "comparison_df.head(10)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

