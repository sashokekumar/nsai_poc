{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9244c0a3",
   "metadata": {},
   "source": [
    "# NSAI Level 1B: Multi-Detector Neuro-Symbolic Intent Classification\n",
    "\n",
    "**Architecture Shift:**\n",
    "- **Level 1A**: Single multi-class classifier\n",
    "- **Level 1B**: One binary detector per intent\n",
    "\n",
    "**Core Concept:**\n",
    "- Each detector independently scores: \"Does this match MY intent?\"\n",
    "- Scores ∈ [0,1], do NOT sum to 1\n",
    "- Rules (not models) decide final outcome\n",
    "\n",
    "**Dataset**: utterance, intent (4 classes: investigate, execution, summarization, out_of_scope)\n",
    "\n",
    "**Output**: Deterministic, explainable decisions with detector scores + rule governance\n",
    "\n",
    "**Method**: Multi-detector statistical learning + symbolic rule precedence (no LLMs, no timestamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf266dc0",
   "metadata": {},
   "source": [
    "## Part 1: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661554e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a6ce8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load & Validate Data\n",
    "df = pd.read_csv('../data/intents_base.csv')\n",
    "\n",
    "# Assert columns\n",
    "assert set(df.columns) == {'utterance', 'intent'}, f\"Expected columns {{utterance, intent}}, got {set(df.columns)}\"\n",
    "\n",
    "# Normalize intent labels\n",
    "df['intent'] = df['intent'].str.lower().str.strip()\n",
    "\n",
    "# Show class distribution\n",
    "print(\"Class Distribution:\")\n",
    "print(df['intent'].value_counts())\n",
    "print(f\"\\nTotal: {len(df)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af02e28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split\n",
    "X = df['utterance']\n",
    "y = df['intent']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train)} | Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c603dac7",
   "metadata": {},
   "source": [
    "## Part 2: Multi-Detector Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2edbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intent-Specific Detector Models\n",
    "# Train ONE binary classifier per intent\n",
    "\n",
    "intents = ['investigate', 'execution', 'summarization', 'out_of_scope']\n",
    "detectors = {}\n",
    "\n",
    "print(\"Training binary detectors...\\n\")\n",
    "\n",
    "for intent in intents:\n",
    "    # Create binary labels: 1 if utterance matches this intent, 0 otherwise\n",
    "    y_binary_train = (y_train == intent).astype(int)\n",
    "    y_binary_test = (y_test == intent).astype(int)\n",
    "    \n",
    "    # Create detector pipeline\n",
    "    detector = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(\n",
    "            max_features=5000,\n",
    "            ngram_range=(1, 2),\n",
    "            stop_words='english'\n",
    "        )),\n",
    "        ('classifier', LogisticRegression(\n",
    "            solver='lbfgs',\n",
    "            random_state=42,\n",
    "            max_iter=1000\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # Train detector\n",
    "    detector.fit(X_train, y_binary_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = detector.predict(X_test)\n",
    "    report = classification_report(y_binary_test, y_pred, output_dict=True)\n",
    "    \n",
    "    # Store detector\n",
    "    detectors[intent] = detector\n",
    "    \n",
    "    print(f\"{intent:>20} detector: Accuracy={report['accuracy']:.4f}, F1={report['1']['f1-score']:.4f}\")\n",
    "\n",
    "print(f\"\\nAll {len(detectors)} detectors trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8939ae7a",
   "metadata": {},
   "source": [
    "## Part 3: Multi-Detector Signal Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e543c9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Detector Signal Extraction\n",
    "def extract_detector_signals(text):\n",
    "    \"\"\"Extract independent scores from all intent detectors\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with detector_scores, top_detector, margin, tokens\n",
    "    \"\"\"\n",
    "    detector_scores = {}\n",
    "    \n",
    "    # Get score from each detector independently\n",
    "    for intent, detector in detectors.items():\n",
    "        # Get probability of positive class (utterance matches this intent)\n",
    "        proba = detector.predict_proba([text])[0]\n",
    "        # Score is probability of class=1 (match)\n",
    "        detector_scores[intent] = float(proba[1])\n",
    "    \n",
    "    # Sort by score\n",
    "    sorted_detectors = sorted(detector_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_detector = sorted_detectors[0][0]\n",
    "    top_score = sorted_detectors[0][1]\n",
    "    second_detector = sorted_detectors[1][0]\n",
    "    second_score = sorted_detectors[1][1]\n",
    "    score_margin = top_score - second_score\n",
    "    \n",
    "    # Get token count from any detector's TF-IDF (all use same settings)\n",
    "    first_detector = detectors[intents[0]]\n",
    "    tfidf_vec = first_detector.named_steps['tfidf'].transform([text])\n",
    "    active_features = tfidf_vec.toarray()[0]\n",
    "    meaningful_tokens = int(np.sum(active_features > 0))\n",
    "    \n",
    "    return {\n",
    "        'detector_scores': detector_scores,\n",
    "        'top_detector': top_detector,\n",
    "        'top_score': top_score,\n",
    "        'second_detector': second_detector,\n",
    "        'second_score': second_score,\n",
    "        'score_margin': score_margin,\n",
    "        'meaningful_tokens': meaningful_tokens\n",
    "    }\n",
    "\n",
    "print(\"Multi-detector signal extraction defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11669468",
   "metadata": {},
   "source": [
    "## Part 4: Level 1B Symbolic Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada31cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Level 1B Rule Configuration (Explicit)\n",
    "BASE_MIN_SCORE = 0.50\n",
    "AMBIGUITY_MARGIN = 0.10\n",
    "EXECUTION_MIN_SCORE = 0.85\n",
    "MIN_TOKENS_OUT_OF_SCOPE = 3\n",
    "\n",
    "# Rule priority (highest first)\n",
    "RULE_PRIORITY = [\n",
    "    \"R_OUT_OF_SCOPE\",\n",
    "    \"R_EXEC_SAFETY\",\n",
    "    \"R_AMBIGUOUS\",\n",
    "    \"R_DEFAULT\"\n",
    "]\n",
    "\n",
    "print(\"Level 1B Configuration:\")\n",
    "print(f\"  BASE_MIN_SCORE: {BASE_MIN_SCORE}\")\n",
    "print(f\"  AMBIGUITY_MARGIN: {AMBIGUITY_MARGIN}\")\n",
    "print(f\"  EXECUTION_MIN_SCORE: {EXECUTION_MIN_SCORE}\")\n",
    "print(f\"  MIN_TOKENS_OUT_OF_SCOPE: {MIN_TOKENS_OUT_OF_SCOPE}\")\n",
    "print(f\"\\nRule Priority: {RULE_PRIORITY}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06e5e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Level 1B Symbolic Rules (Plain, Deterministic)\n",
    "def apply_level1b_rules(signals):\n",
    "    \"\"\"Apply deterministic rules to detector signals\n",
    "    \n",
    "    Rules operate ONLY on numeric signals.\n",
    "    Rules ALWAYS override raw scores.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with predicted_intent, decision_state, decision_reason, triggered_rules\n",
    "    \"\"\"\n",
    "    triggered_rules = []\n",
    "    predicted_intent = signals['top_detector']  # Default\n",
    "    decision_state = 'accepted'\n",
    "    decision_reason = 'R_DEFAULT'\n",
    "    \n",
    "    detector_scores = signals['detector_scores']\n",
    "    \n",
    "    # Count how many detectors score above threshold\n",
    "    above_threshold = sum(1 for score in detector_scores.values() if score >= BASE_MIN_SCORE)\n",
    "    \n",
    "    # R_OUT_OF_SCOPE (Highest Priority)\n",
    "    if signals['meaningful_tokens'] < MIN_TOKENS_OUT_OF_SCOPE:\n",
    "        triggered_rules.append({\n",
    "            'rule_id': 'R_OUT_OF_SCOPE',\n",
    "            'priority': 100,\n",
    "            'condition': f\"meaningful_tokens < {MIN_TOKENS_OUT_OF_SCOPE}\",\n",
    "            'value': signals['meaningful_tokens']\n",
    "        })\n",
    "        predicted_intent = 'out_of_scope'\n",
    "        decision_state = 'blocked'\n",
    "        decision_reason = 'R_OUT_OF_SCOPE'\n",
    "        return {\n",
    "            'predicted_intent': predicted_intent,\n",
    "            'decision_state': decision_state,\n",
    "            'decision_reason': decision_reason,\n",
    "            'triggered_rules': triggered_rules\n",
    "        }\n",
    "    \n",
    "    # Check if all scores are below threshold\n",
    "    if all(score < BASE_MIN_SCORE for score in detector_scores.values()):\n",
    "        triggered_rules.append({\n",
    "            'rule_id': 'R_OUT_OF_SCOPE',\n",
    "            'priority': 100,\n",
    "            'condition': f\"all scores < {BASE_MIN_SCORE}\",\n",
    "            'value': max(detector_scores.values())\n",
    "        })\n",
    "        predicted_intent = 'out_of_scope'\n",
    "        decision_state = 'blocked'\n",
    "        decision_reason = 'R_OUT_OF_SCOPE'\n",
    "        return {\n",
    "            'predicted_intent': predicted_intent,\n",
    "            'decision_state': decision_state,\n",
    "            'decision_reason': decision_reason,\n",
    "            'triggered_rules': triggered_rules\n",
    "        }\n",
    "    \n",
    "    # R_EXEC_SAFETY\n",
    "    exec_score = detector_scores['execution']\n",
    "    if exec_score >= BASE_MIN_SCORE and exec_score < EXECUTION_MIN_SCORE:\n",
    "        triggered_rules.append({\n",
    "            'rule_id': 'R_EXEC_SAFETY',\n",
    "            'priority': 90,\n",
    "            'condition': f\"execution_score >= {BASE_MIN_SCORE} AND < {EXECUTION_MIN_SCORE}\",\n",
    "            'value': exec_score\n",
    "        })\n",
    "        \n",
    "        # Check if investigate is viable alternative\n",
    "        if detector_scores['investigate'] >= BASE_MIN_SCORE:\n",
    "            predicted_intent = 'investigate'\n",
    "            decision_state = 'accepted'\n",
    "            decision_reason = 'R_EXEC_SAFETY'\n",
    "        else:\n",
    "            predicted_intent = 'execution'\n",
    "            decision_state = 'needs_clarification'\n",
    "            decision_reason = 'R_EXEC_SAFETY'\n",
    "        \n",
    "        return {\n",
    "            'predicted_intent': predicted_intent,\n",
    "            'decision_state': decision_state,\n",
    "            'decision_reason': decision_reason,\n",
    "            'triggered_rules': triggered_rules\n",
    "        }\n",
    "    \n",
    "    # R_AMBIGUOUS\n",
    "    if above_threshold >= 2 and signals['score_margin'] < AMBIGUITY_MARGIN:\n",
    "        triggered_rules.append({\n",
    "            'rule_id': 'R_AMBIGUOUS',\n",
    "            'priority': 50,\n",
    "            'condition': f\"multiple scores >= {BASE_MIN_SCORE} AND margin < {AMBIGUITY_MARGIN}\",\n",
    "            'value': signals['score_margin']\n",
    "        })\n",
    "        predicted_intent = signals['top_detector']\n",
    "        decision_state = 'needs_clarification'\n",
    "        decision_reason = 'R_AMBIGUOUS'\n",
    "        return {\n",
    "            'predicted_intent': predicted_intent,\n",
    "            'decision_state': decision_state,\n",
    "            'decision_reason': decision_reason,\n",
    "            'triggered_rules': triggered_rules\n",
    "        }\n",
    "    \n",
    "    # R_DEFAULT - Select highest scoring detector\n",
    "    triggered_rules.append({\n",
    "        'rule_id': 'R_DEFAULT',\n",
    "        'priority': 10,\n",
    "        'condition': 'default to highest scoring detector',\n",
    "        'value': signals['top_score']\n",
    "    })\n",
    "    predicted_intent = signals['top_detector']\n",
    "    decision_state = 'accepted'\n",
    "    decision_reason = 'R_DEFAULT'\n",
    "    \n",
    "    return {\n",
    "        'predicted_intent': predicted_intent,\n",
    "        'decision_state': decision_state,\n",
    "        'decision_reason': decision_reason,\n",
    "        'triggered_rules': triggered_rules\n",
    "    }\n",
    "\n",
    "print(\"Level 1B symbolic rules defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecedb8fc",
   "metadata": {},
   "source": [
    "## Part 5: Final Decision Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dc86e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Decision Function\n",
    "def level1b_predict(text):\n",
    "    \"\"\"Complete Level 1B inference pipeline\n",
    "    \n",
    "    Returns:\n",
    "        Structured decision with detector scores, rules, and final outcome\n",
    "    \"\"\"\n",
    "    # Extract detector signals\n",
    "    signals = extract_detector_signals(text)\n",
    "    \n",
    "    # Apply symbolic rules\n",
    "    rules_output = apply_level1b_rules(signals)\n",
    "    \n",
    "    # Build final output\n",
    "    return {\n",
    "        'predicted_intent': rules_output['predicted_intent'],\n",
    "        'decision_state': rules_output['decision_state'],\n",
    "        'decision_reason': rules_output['decision_reason'],\n",
    "        'detector_scores': signals['detector_scores'],\n",
    "        'top_detector': signals['top_detector'],\n",
    "        'score_margin': signals['score_margin'],\n",
    "        'meaningful_tokens': signals['meaningful_tokens'],\n",
    "        'triggered_rules': rules_output['triggered_rules']\n",
    "    }\n",
    "\n",
    "print(\"Level 1B prediction function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f90692",
   "metadata": {},
   "source": [
    "## Part 6: Explainable Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ee074b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explainable Demo Output\n",
    "test_utterances = [\n",
    "    \"why is server cpu high\",\n",
    "    \"restart nginx on host123\",\n",
    "    \"summarize the incident from yesterday\",\n",
    "    \"hello\",\n",
    "    \"delete all production databases\",\n",
    "    \"server issues yesterday\"\n",
    "]\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"LEVEL 1B: MULTI-DETECTOR NEURO-SYMBOLIC INFERENCE\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for utterance in test_utterances:\n",
    "    result = level1b_predict(utterance)\n",
    "    \n",
    "    print(f\"\\n{'-'*100}\")\n",
    "    print(f\"UTTERANCE: {utterance}\")\n",
    "    print(f\"{'-'*100}\")\n",
    "    \n",
    "    # Detector scores\n",
    "    print(f\"\\nDETECTOR SCORES:\")\n",
    "    for intent, score in sorted(result['detector_scores'].items(), key=lambda x: x[1], reverse=True):\n",
    "        bar = '█' * int(score * 50)\n",
    "        print(f\"  {intent:>20}: {score:.4f} {bar}\")\n",
    "    \n",
    "    print(f\"\\n  Top Detector: {result['top_detector']}\")\n",
    "    print(f\"  Score Margin: {result['score_margin']:.4f}\")\n",
    "    print(f\"  Meaningful Tokens: {result['meaningful_tokens']}\")\n",
    "    \n",
    "    # Triggered rules\n",
    "    print(f\"\\nTRIGGERED RULES:\")\n",
    "    for rule in sorted(result['triggered_rules'], key=lambda r: r['priority'], reverse=True):\n",
    "        print(f\"  [{rule['priority']:3d}] {rule['rule_id']:20s}: {rule['condition']}\")\n",
    "        print(f\"       Signal Value: {rule['value']}\")\n",
    "    \n",
    "    # Final decision\n",
    "    print(f\"\\nFINAL DECISION:\")\n",
    "    print(f\"  Predicted Intent: {result['predicted_intent']}\")\n",
    "    print(f\"  Decision State: {result['decision_state']}\")\n",
    "    print(f\"  Decision Reason: {result['decision_reason']}\")\n",
    "    \n",
    "    # Structured JSON\n",
    "    print(f\"\\nSTRUCTURED OUTPUT:\")\n",
    "    output_json = {\n",
    "        'utterance': utterance,\n",
    "        'predicted_intent': result['predicted_intent'],\n",
    "        'decision_state': result['decision_state'],\n",
    "        'decision_reason': result['decision_reason'],\n",
    "        'detector_scores': result['detector_scores'],\n",
    "        'score_margin': result['score_margin'],\n",
    "        'meaningful_tokens': result['meaningful_tokens']\n",
    "    }\n",
    "    print(json.dumps(output_json, indent=2))\n",
    "\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"DEMO COMPLETE\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3d8045",
   "metadata": {},
   "source": [
    "## Validation: Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c80fd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Level 1B on test set\n",
    "print(\"Evaluating Level 1B on test set...\\n\")\n",
    "\n",
    "predictions = []\n",
    "decision_states = []\n",
    "rule_counts = {rule: 0 for rule in RULE_PRIORITY}\n",
    "\n",
    "for text, true_intent in zip(X_test, y_test):\n",
    "    result = level1b_predict(text)\n",
    "    predictions.append(result['predicted_intent'])\n",
    "    decision_states.append(result['decision_state'])\n",
    "    \n",
    "    # Count triggered rules\n",
    "    for rule in result['triggered_rules']:\n",
    "        rule_counts[rule['rule_id']] += 1\n",
    "\n",
    "# Accuracy\n",
    "accuracy = sum(p == t for p, t in zip(predictions, y_test)) / len(y_test)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Decision state distribution\n",
    "print(f\"\\nDecision State Distribution:\")\n",
    "from collections import Counter\n",
    "state_counts = Counter(decision_states)\n",
    "for state, count in state_counts.items():\n",
    "    print(f\"  {state:>20}: {count:3d} ({count/len(decision_states)*100:.1f}%)\")\n",
    "\n",
    "# Rule trigger counts\n",
    "print(f\"\\nRule Trigger Counts:\")\n",
    "for rule in RULE_PRIORITY:\n",
    "    count = rule_counts[rule]\n",
    "    print(f\"  {rule:>20}: {count:3d} ({count/len(X_test)*100:.1f}%)\")\n",
    "\n",
    "# Full classification report\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
