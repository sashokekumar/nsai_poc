{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8548839d",
   "metadata": {},
   "source": [
    "# NSAI Level 1: Neuro-Symbolic Intent Classification\n",
    "\n",
    "**2-Layer Architecture:**\n",
    "- **Layer A (Statistical)**: TF-IDF + Logistic Regression for intent prediction\n",
    "- **Layer B (Neuro-Symbolic)**: Deterministic rules with strict precedence that gate or override statistical output\n",
    "\n",
    "**Dataset**: utterance, intent (4 classes: investigate, execution, summarization, out_of_scope)\n",
    "\n",
    "**Output**: Structured, explainable decisions with signals + triggered rules + final intent\n",
    "\n",
    "**Method**: Pure statistical learning + symbolic rule precedence (no LLMs, no timestamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7148cfc1",
   "metadata": {},
   "source": [
    "## Part 1: Statistical Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5b09efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0392a1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution:\n",
      "intent\n",
      "out_of_scope     169\n",
      "execution        150\n",
      "investigate      149\n",
      "summarization    146\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total: 614 records\n"
     ]
    }
   ],
   "source": [
    "# Load & Validate Data\n",
    "df = pd.read_csv('../data/intents_base.csv')\n",
    "\n",
    "# Assert columns\n",
    "assert set(df.columns) == {'utterance', 'intent'}, f\"Expected columns {{utterance, intent}}, got {set(df.columns)}\"\n",
    "\n",
    "# Normalize intent labels\n",
    "df['intent'] = df['intent'].str.lower().str.strip()\n",
    "\n",
    "# Show class distribution\n",
    "print(\"Class Distribution:\")\n",
    "print(df['intent'].value_counts())\n",
    "print(f\"\\nTotal: {len(df)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1cbe290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 491 | Test: 123\n"
     ]
    }
   ],
   "source": [
    "# Train/Test Split\n",
    "X = df['utterance']\n",
    "y = df['intent']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train)} | Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "008e9e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained\n"
     ]
    }
   ],
   "source": [
    "# Level-1 Model\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        max_features=5000,\n",
    "        ngram_range=(1, 2),\n",
    "        stop_words='english'\n",
    "    )),\n",
    "    ('classifier', LogisticRegression(\n",
    "        solver='lbfgs',\n",
    "        random_state=42,\n",
    "        max_iter=1000\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Fit\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"Model trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a38730ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    execution       0.80      0.93      0.86        30\n",
      "  investigate       0.92      0.73      0.81        30\n",
      " out_of_scope       0.94      1.00      0.97        34\n",
      "summarization       1.00      0.97      0.98        29\n",
      "\n",
      "     accuracy                           0.91       123\n",
      "    macro avg       0.92      0.91      0.91       123\n",
      " weighted avg       0.92      0.91      0.91       123\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[22  7  0  1]\n",
      " [ 1 28  0  1]\n",
      " [ 1  0 28  0]\n",
      " [ 0  0  0 34]]\n",
      "\n",
      "Labels: ['investigate', 'execution', 'summarization', 'out_of_scope']\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred, labels=['investigate', 'execution', 'summarization', 'out_of_scope']))\n",
    "print(\"\\nLabels: ['investigate', 'execution', 'summarization', 'out_of_scope']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a80170c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence extraction function defined\n"
     ]
    }
   ],
   "source": [
    "# Token-Level Evidence\n",
    "def predict_with_evidence(text, top_k=5):\n",
    "    \"\"\"Predict intent and extract top contributing tokens\"\"\"\n",
    "    # Predict\n",
    "    pred_intent = pipeline.predict([text])[0]\n",
    "    proba = pipeline.predict_proba([text])[0]\n",
    "    confidence = float(np.max(proba))\n",
    "    \n",
    "    # Get class index\n",
    "    classes = pipeline.named_steps['classifier'].classes_\n",
    "    class_idx = np.where(classes == pred_intent)[0][0]\n",
    "    \n",
    "    # Get coefficients for predicted class\n",
    "    coef = pipeline.named_steps['classifier'].coef_[class_idx]\n",
    "    \n",
    "    # Get feature names\n",
    "    feature_names = pipeline.named_steps['tfidf'].get_feature_names_out()\n",
    "    \n",
    "    # Transform text to get active features\n",
    "    tfidf_vec = pipeline.named_steps['tfidf'].transform([text])\n",
    "    active_features = tfidf_vec.toarray()[0]\n",
    "    \n",
    "    # Calculate contributions\n",
    "    contributions = active_features * coef\n",
    "    \n",
    "    # Get top k\n",
    "    top_indices = np.argsort(contributions)[-top_k:][::-1]\n",
    "    \n",
    "    evidence = []\n",
    "    for idx in top_indices:\n",
    "        if active_features[idx] > 0:\n",
    "            evidence.append({\n",
    "                'token': feature_names[idx],\n",
    "                'weight': float(contributions[idx])\n",
    "            })\n",
    "    \n",
    "    return {\n",
    "        'predicted_intent': pred_intent,\n",
    "        'confidence': confidence,\n",
    "        'evidence_tokens': evidence\n",
    "    }\n",
    "\n",
    "print(\"Evidence extraction function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24ababd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Utterance: why is server cpu high\n",
      "Predicted: investigate\n",
      "Confidence: 0.5534\n",
      "Evidence: [{'token': 'high', 'weight': 0.3994956699594776}, {'token': 'cpu', 'weight': 0.37353200156682}, {'token': 'cpu high', 'weight': 0.12062450706017938}, {'token': 'server', 'weight': 0.1170763458242249}]\n",
      "\n",
      "Utterance: restart nginx on host123\n",
      "Predicted: execution\n",
      "Confidence: 0.5797\n",
      "Evidence: [{'token': 'restart', 'weight': 0.6980231193486794}, {'token': 'restart nginx', 'weight': 0.09781387842338801}, {'token': 'nginx host123', 'weight': 0.09781387842338801}, {'token': 'nginx', 'weight': 0.09781387842338801}, {'token': 'host123', 'weight': 0.05029071292704667}]\n",
      "\n",
      "Utterance: summarize the incident from yesterday\n",
      "Predicted: summarization\n",
      "Confidence: 0.7562\n",
      "Evidence: [{'token': 'summarize', 'weight': 1.3179584121997834}, {'token': 'incident', 'weight': 0.37610919040396457}, {'token': 'summarize incident', 'weight': 0.20255519935166402}, {'token': 'yesterday', 'weight': 0.08769134009940947}]\n"
     ]
    }
   ],
   "source": [
    "# Sanity Checks\n",
    "test_cases = [\n",
    "    \"why is server cpu high\",\n",
    "    \"restart nginx on host123\",\n",
    "    \"summarize the incident from yesterday\"\n",
    "]\n",
    "\n",
    "for utterance in test_cases:\n",
    "    result = predict_with_evidence(utterance)\n",
    "    print(f\"\\nUtterance: {utterance}\")\n",
    "    print(f\"Predicted: {result['predicted_intent']}\")\n",
    "    print(f\"Confidence: {result['confidence']:.4f}\")\n",
    "    print(f\"Evidence: {result['evidence_tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc9302b",
   "metadata": {},
   "source": [
    "## Part 2: NSAI Symbolic Decision Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daab2406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "  BASE_MIN_CONF = 0.6\n",
      "  MIN_MARGIN = 0.1\n",
      "  EXECUTION_MIN_CONF = 0.85\n",
      "  MIN_TOKENS_OUT_OF_SCOPE = 3\n",
      "  RANDOM_STATE = 42\n",
      "  TEST_SIZE = 0.2\n"
     ]
    }
   ],
   "source": [
    "# Configuration Constants\n",
    "CONFIG = {\n",
    "    'BASE_MIN_CONF': 0.60,           # Minimum confidence for accepting prediction\n",
    "    'MIN_MARGIN': 0.10,              # Minimum margin between top-2 predictions\n",
    "    'EXECUTION_MIN_CONF': 0.85,      # Higher bar for execution intent\n",
    "    'MIN_TOKENS_OUT_OF_SCOPE': 3,    # Token count threshold for out_of_scope\n",
    "    'RANDOM_STATE': 42,              # For reproducibility\n",
    "    'TEST_SIZE': 0.2                 # Train/test split ratio\n",
    "}\n",
    "\n",
    "# Explicit Rule Priority (highest to lowest)\n",
    "RULE_PRIORITY = [\"R1\", \"R4\", \"R2\", \"R3\"]\n",
    "\n",
    "\n",
    "print(\"Configuration loaded:\")print(f\"\\nRule Priority: {RULE_PRIORITY}\")\n",
    "\n",
    "for key, val in CONFIG.items():    print(f\"  {key} = {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b42e22c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signal extraction function defined\n"
     ]
    }
   ],
   "source": [
    "# Signal Extraction\n",
    "def extract_signals(text, top_k=5):\n",
    "    \"\"\"Extract all signals from the model for a given utterance\"\"\"\n",
    "    # Get probabilities\n",
    "    proba = pipeline.predict_proba([text])[0]\n",
    "    classes = pipeline.named_steps['classifier'].classes_\n",
    "    \n",
    "    # Sort by probability\n",
    "    sorted_indices = np.argsort(proba)[::-1]\n",
    "    max_confidence = float(proba[sorted_indices[0]])\n",
    "    second_best_confidence = float(proba[sorted_indices[1]])\n",
    "    margin = max_confidence - second_best_confidence\n",
    "    \n",
    "    # Get TF-IDF representation\n",
    "    tfidf_vec = pipeline.named_steps['tfidf'].transform([text])\n",
    "    active_features = tfidf_vec.toarray()[0]\n",
    "    feature_names = pipeline.named_steps['tfidf'].get_feature_names_out()\n",
    "    \n",
    "    # Count meaningful tokens (non-zero TF-IDF)\n",
    "    meaningful_tokens = np.sum(active_features > 0)\n",
    "    \n",
    "    # Extract top contributing tokens per intent\n",
    "    top_tokens_per_intent = {}\n",
    "    for intent_idx, intent in enumerate(classes):\n",
    "        coef = pipeline.named_steps['classifier'].coef_[intent_idx]\n",
    "        contributions = active_features * coef\n",
    "        top_indices = np.argsort(contributions)[-top_k:][::-1]\n",
    "        \n",
    "        tokens = []\n",
    "        for idx in top_indices:\n",
    "            if active_features[idx] > 0:\n",
    "                tokens.append({\n",
    "                    'token': feature_names[idx],\n",
    "                    'weight': float(contributions[idx])\n",
    "                })\n",
    "        top_tokens_per_intent[intent] = tokens\n",
    "    \n",
    "    # Build signals dictionary\n",
    "    signals = {\n",
    "        'probabilities': {intent: float(proba[i]) for i, intent in enumerate(classes)},\n",
    "        'max_confidence': max_confidence,\n",
    "        'second_best_confidence': second_best_confidence,\n",
    "        'margin': margin,\n",
    "        'predicted_intent': classes[sorted_indices[0]],\n",
    "        'meaningful_tokens': int(meaningful_tokens),\n",
    "        'top_tokens_per_intent': top_tokens_per_intent\n",
    "    }\n",
    "    \n",
    "    return signals\n",
    "\n",
    "print(\"Signal extraction function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "45cf261f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision rules with strict precedence defined\n"
     ]
    }
   ],
   "source": [
    "# NSAI Decision Rules with Strict Precedence\n",
    "def apply_decision_rules(signals):\n",
    "    \"\"\"Apply deterministic rules with strict precedence hierarchy\n",
    "    \n",
    "    Intent = what user wants (always from model)\n",
    "    Decision State = what system decides to do\n",
    "    \n",
    "    Rule Categories:\n",
    "    - quality: input sufficiency (R1)\n",
    "    - safety: execution controls (R4)\n",
    "    - ambiguity: confidence/margin (R2, R3)\n",
    "    \n",
    "    Precedence follows RULE_PRIORITY: R1 > R4 > R2 > R3\n",
    "    \"\"\"\n",
    "    triggered_rules = []\n",
    "    predicted_intent = signals['predicted_intent']  # Always keep model prediction\n",
    "    decision_state = 'accepted'\n",
    "    decision_reason = 'model_prediction'\n",
    "    \n",
    "    # R1: QUALITY GATE (highest priority)\n",
    "    if signals['meaningful_tokens'] < CONFIG['MIN_TOKENS_OUT_OF_SCOPE']:\n",
    "        triggered_rules.append({\n",
    "            'rule_id': 'R1',\n",
    "            'category': 'quality',\n",
    "            'priority': 100,\n",
    "            'condition': f\"meaningful_tokens < {CONFIG['MIN_TOKENS_OUT_OF_SCOPE']}\",\n",
    "            'signal_value': signals['meaningful_tokens']\n",
    "        })\n",
    "        decision_state = 'blocked'\n",
    "        decision_reason = 'insufficient_tokens'\n",
    "        # R1 overrides everything - return immediately\n",
    "        return {\n",
    "            'triggered_rules': triggered_rules,\n",
    "            'predicted_intent': predicted_intent,\n",
    "            'decision_state': decision_state,\n",
    "            'decision_reason': decision_reason\n",
    "        }\n",
    "    \n",
    "    # R4: SAFETY GATE\n",
    "    if signals['predicted_intent'] == 'execution' and signals['max_confidence'] < CONFIG['EXECUTION_MIN_CONF']:\n",
    "        triggered_rules.append({\n",
    "            'rule_id': 'R4',\n",
    "            'category': 'safety',\n",
    "            'priority': 90,\n",
    "            'condition': f\"predicted_intent==execution AND max_confidence < {CONFIG['EXECUTION_MIN_CONF']}\",\n",
    "            'signal_value': signals['max_confidence']\n",
    "        })\n",
    "        # Block execution if confidence insufficient\n",
    "        if signals['max_confidence'] >= CONFIG['BASE_MIN_CONF']:\n",
    "            decision_state = 'blocked'\n",
    "            decision_reason = 'execution_safety_block'\n",
    "        else:\n",
    "            decision_state = 'needs_clarification'\n",
    "            decision_reason = 'execution_low_confidence'\n",
    "        # R4 overrides R2 and R3 - return immediately\n",
    "        return {\n",
    "            'triggered_rules': triggered_rules,\n",
    "            'predicted_intent': predicted_intent,\n",
    "            'decision_state': decision_state,\n",
    "            'decision_reason': decision_reason\n",
    "        }\n",
    "    \n",
    "    # R2 & R3: AMBIGUITY GATES\n",
    "    ambiguity_detected = False\n",
    "    \n",
    "    # R2: Low confidence\n",
    "    if signals['max_confidence'] < CONFIG['BASE_MIN_CONF']:\n",
    "        triggered_rules.append({\n",
    "            'rule_id': 'R2',\n",
    "            'category': 'ambiguity',\n",
    "            'priority': 50,\n",
    "            'condition': f\"max_confidence < {CONFIG['BASE_MIN_CONF']}\",\n",
    "            'signal_value': signals['max_confidence']\n",
    "        })\n",
    "        ambiguity_detected = True\n",
    "    \n",
    "    # R3: Low margin\n",
    "    if signals['margin'] < CONFIG['MIN_MARGIN']:\n",
    "        triggered_rules.append({\n",
    "            'rule_id': 'R3',\n",
    "            'category': 'ambiguity',\n",
    "            'priority': 40,\n",
    "            'condition': f\"margin < {CONFIG['MIN_MARGIN']}\",\n",
    "            'signal_value': signals['margin']\n",
    "        })\n",
    "        ambiguity_detected = True\n",
    "    \n",
    "    if ambiguity_detected:\n",
    "        decision_state = 'needs_clarification'\n",
    "        decision_reason = 'ambiguous_prediction'\n",
    "    \n",
    "    return {\n",
    "        'triggered_rules': triggered_rules,\n",
    "        'predicted_intent': predicted_intent,\n",
    "        'decision_state': decision_state,\n",
    "        'decision_reason': decision_reason\n",
    "    }\n",
    "\n",
    "print(\"Decision rules with strict precedence defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06dc815a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transparent voting function defined\n"
     ]
    }
   ],
   "source": [
    "# Signal Voting Aggregation (Transparent Scoring)\n",
    "def weighted_voting(signals):\n",
    "    \"\"\"Compute vote scores for transparency\n",
    "    \n",
    "    NOTE: Votes are for explainability only.\n",
    "    Rules have final authority and override votes.\n",
    "    \"\"\"\n",
    "    scores = {intent: 0.0 for intent in signals['probabilities'].keys()}\n",
    "    vote_log = []\n",
    "    \n",
    "    # Vote 1: Base probability scores\n",
    "    for intent, prob in signals['probabilities'].items():\n",
    "        scores[intent] += prob * 1.0\n",
    "        vote_log.append({\n",
    "            'vote_id': 'V1',\n",
    "            'source': 'model_probability',\n",
    "            'intent': intent,\n",
    "            'contribution': prob * 1.0\n",
    "        })\n",
    "    \n",
    "    # Vote 2: Confidence bonus for high-confidence predictions\n",
    "    if signals['max_confidence'] > CONFIG['EXECUTION_MIN_CONF']:\n",
    "        bonus = 0.3\n",
    "        scores[signals['predicted_intent']] += bonus\n",
    "        vote_log.append({\n",
    "            'vote_id': 'V2',\n",
    "            'source': 'high_confidence_bonus',\n",
    "            'intent': signals['predicted_intent'],\n",
    "            'contribution': bonus\n",
    "        })\n",
    "    \n",
    "    # Vote 3: Margin bonus\n",
    "    if signals['margin'] > 0.20:\n",
    "        bonus = 0.2\n",
    "        scores[signals['predicted_intent']] += bonus\n",
    "        vote_log.append({\n",
    "            'vote_id': 'V3',\n",
    "            'source': 'high_margin_bonus',\n",
    "            'intent': signals['predicted_intent'],\n",
    "            'contribution': bonus\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        'score_per_intent': scores,\n",
    "        'vote_log': vote_log,\n",
    "        'vote_winner': max(scores, key=scores.get)\n",
    "    }\n",
    "\n",
    "print(\"Transparent voting function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1cbf540c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "UTTERANCE: restart nginx on host123\n",
      "================================================================================\n",
      "\n",
      "STRUCTURED OUTPUT:\n",
      "{\n",
      "  \"utterance\": \"restart nginx on host123\",\n",
      "  \"predicted_intent\": \"execution\",\n",
      "  \"signals\": {\n",
      "    \"max_confidence\": 0.5796926800149172,\n",
      "    \"margin\": 0.41430265968025004,\n",
      "    \"meaningful_tokens\": 5,\n",
      "    \"probabilities\": {\n",
      "      \"execution\": 0.5796926800149172,\n",
      "      \"investigate\": 0.1494487335143245,\n",
      "      \"out_of_scope\": 0.16539002033466724,\n",
      "      \"summarization\": 0.10546856613609105\n",
      "    }\n",
      "  },\n",
      "  \"triggered_rules\": [\n",
      "    {\n",
      "      \"rule_id\": \"R4\",\n",
      "      \"category\": \"safety\",\n",
      "      \"priority\": 90,\n",
      "      \"condition\": \"predicted_intent==execution AND max_confidence < 0.85\",\n",
      "      \"value\": 0.5796926800149172\n",
      "    }\n",
      "  ],\n",
      "  \"decision_state\": \"needs_clarification\",\n",
      "  \"decision_reason\": \"execution_low_confidence\"\n",
      "}\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "LAYER A — STATISTICAL MODEL:\n",
      "  predicted_intent: execution\n",
      "  max_confidence: 0.5797\n",
      "  margin: 0.4143\n",
      "  meaningful_tokens: 5\n",
      "\n",
      "LAYER B — NEURO-SYMBOLIC RULES:\n",
      "  Triggered: 1 rule(s)\n",
      "    [P90] R4: predicted_intent==execution AND max_confidence < 0.85 (value=0.5797)\n",
      "\n",
      "TRANSPARENT VOTING (for reference):\n",
      "  vote_winner: execution\n",
      "    execution: 0.7797\n",
      "    out_of_scope: 0.1654\n",
      "    investigate: 0.1494\n",
      "    summarization: 0.1055\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FINAL DECISION:\n",
      "  predicted_intent: execution\n",
      "  decision_state: needs_clarification\n",
      "  decision_reason: execution_low_confidence\n",
      "  ↳ Decision controlled by R4 (safety)\n",
      "\n",
      "================================================================================\n",
      "UTTERANCE: why is server cpu high\n",
      "================================================================================\n",
      "\n",
      "STRUCTURED OUTPUT:\n",
      "{\n",
      "  \"utterance\": \"why is server cpu high\",\n",
      "  \"predicted_intent\": \"investigate\",\n",
      "  \"signals\": {\n",
      "    \"max_confidence\": 0.5533671565703482,\n",
      "    \"margin\": 0.35784094727836235,\n",
      "    \"meaningful_tokens\": 4,\n",
      "    \"probabilities\": {\n",
      "      \"execution\": 0.19552620929198583,\n",
      "      \"investigate\": 0.5533671565703482,\n",
      "      \"out_of_scope\": 0.14748693738793223,\n",
      "      \"summarization\": 0.10361969674973384\n",
      "    }\n",
      "  },\n",
      "  \"triggered_rules\": [\n",
      "    {\n",
      "      \"rule_id\": \"R2\",\n",
      "      \"category\": \"ambiguity\",\n",
      "      \"priority\": 50,\n",
      "      \"condition\": \"max_confidence < 0.6\",\n",
      "      \"value\": 0.5533671565703482\n",
      "    }\n",
      "  ],\n",
      "  \"decision_state\": \"needs_clarification\",\n",
      "  \"decision_reason\": \"ambiguous_prediction\"\n",
      "}\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "LAYER A — STATISTICAL MODEL:\n",
      "  predicted_intent: investigate\n",
      "  max_confidence: 0.5534\n",
      "  margin: 0.3578\n",
      "  meaningful_tokens: 4\n",
      "\n",
      "LAYER B — NEURO-SYMBOLIC RULES:\n",
      "  Triggered: 1 rule(s)\n",
      "    [P50] R2: max_confidence < 0.6 (value=0.5534)\n",
      "\n",
      "TRANSPARENT VOTING (for reference):\n",
      "  vote_winner: investigate\n",
      "    investigate: 0.7534\n",
      "    execution: 0.1955\n",
      "    out_of_scope: 0.1475\n",
      "    summarization: 0.1036\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FINAL DECISION:\n",
      "  predicted_intent: investigate\n",
      "  decision_state: needs_clarification\n",
      "  decision_reason: ambiguous_prediction\n",
      "  ↳ Decision controlled by R2 (ambiguity)\n",
      "\n",
      "================================================================================\n",
      "UTTERANCE: hello\n",
      "================================================================================\n",
      "\n",
      "STRUCTURED OUTPUT:\n",
      "{\n",
      "  \"utterance\": \"hello\",\n",
      "  \"predicted_intent\": \"out_of_scope\",\n",
      "  \"signals\": {\n",
      "    \"max_confidence\": 0.48559670968656193,\n",
      "    \"margin\": 0.29597487354866736,\n",
      "    \"meaningful_tokens\": 1,\n",
      "    \"probabilities\": {\n",
      "      \"execution\": 0.18962183613789457,\n",
      "      \"investigate\": 0.18369983182152255,\n",
      "      \"out_of_scope\": 0.48559670968656193,\n",
      "      \"summarization\": 0.14108162235402097\n",
      "    }\n",
      "  },\n",
      "  \"triggered_rules\": [\n",
      "    {\n",
      "      \"rule_id\": \"R1\",\n",
      "      \"category\": \"quality\",\n",
      "      \"priority\": 100,\n",
      "      \"condition\": \"meaningful_tokens < 3\",\n",
      "      \"value\": 1\n",
      "    }\n",
      "  ],\n",
      "  \"decision_state\": \"blocked\",\n",
      "  \"decision_reason\": \"insufficient_tokens\"\n",
      "}\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "LAYER A — STATISTICAL MODEL:\n",
      "  predicted_intent: out_of_scope\n",
      "  max_confidence: 0.4856\n",
      "  margin: 0.2960\n",
      "  meaningful_tokens: 1\n",
      "\n",
      "LAYER B — NEURO-SYMBOLIC RULES:\n",
      "  Triggered: 1 rule(s)\n",
      "    [P100] R1: meaningful_tokens < 3 (value=1.0000)\n",
      "\n",
      "TRANSPARENT VOTING (for reference):\n",
      "  vote_winner: out_of_scope\n",
      "    out_of_scope: 0.6856\n",
      "    execution: 0.1896\n",
      "    investigate: 0.1837\n",
      "    summarization: 0.1411\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FINAL DECISION:\n",
      "  predicted_intent: out_of_scope\n",
      "  decision_state: blocked\n",
      "  decision_reason: insufficient_tokens\n",
      "  ↳ Decision controlled by R1 (quality)\n",
      "\n",
      "================================================================================\n",
      "UTTERANCE: summarize yesterday's incidents\n",
      "================================================================================\n",
      "\n",
      "STRUCTURED OUTPUT:\n",
      "{\n",
      "  \"utterance\": \"summarize yesterday's incidents\",\n",
      "  \"predicted_intent\": \"summarization\",\n",
      "  \"signals\": {\n",
      "    \"max_confidence\": 0.7783414655302809,\n",
      "    \"margin\": 0.6934958694085673,\n",
      "    \"meaningful_tokens\": 2,\n",
      "    \"probabilities\": {\n",
      "      \"execution\": 0.06830813914109128,\n",
      "      \"investigate\": 0.06850479920691423,\n",
      "      \"out_of_scope\": 0.08484559612171359,\n",
      "      \"summarization\": 0.7783414655302809\n",
      "    }\n",
      "  },\n",
      "  \"triggered_rules\": [\n",
      "    {\n",
      "      \"rule_id\": \"R1\",\n",
      "      \"category\": \"quality\",\n",
      "      \"priority\": 100,\n",
      "      \"condition\": \"meaningful_tokens < 3\",\n",
      "      \"value\": 2\n",
      "    }\n",
      "  ],\n",
      "  \"decision_state\": \"blocked\",\n",
      "  \"decision_reason\": \"insufficient_tokens\"\n",
      "}\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "LAYER A — STATISTICAL MODEL:\n",
      "  predicted_intent: summarization\n",
      "  max_confidence: 0.7783\n",
      "  margin: 0.6935\n",
      "  meaningful_tokens: 2\n",
      "\n",
      "LAYER B — NEURO-SYMBOLIC RULES:\n",
      "  Triggered: 1 rule(s)\n",
      "    [P100] R1: meaningful_tokens < 3 (value=2.0000)\n",
      "\n",
      "TRANSPARENT VOTING (for reference):\n",
      "  vote_winner: summarization\n",
      "    summarization: 0.9783\n",
      "    out_of_scope: 0.0848\n",
      "    investigate: 0.0685\n",
      "    execution: 0.0683\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FINAL DECISION:\n",
      "  predicted_intent: summarization\n",
      "  decision_state: blocked\n",
      "  decision_reason: insufficient_tokens\n",
      "  ↳ Decision controlled by R1 (quality)\n"
     ]
    }
   ],
   "source": [
    "# Demo: Explainable Inference\n",
    "import json\n",
    "\n",
    "test_utterances = [\n",
    "    \"restart nginx on host123\",\n",
    "    \"why is server cpu high\",\n",
    "    \"hello\",\n",
    "    \"summarize yesterday's incidents\"\n",
    "]\n",
    "\n",
    "for utterance in test_utterances:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"UTTERANCE: {utterance}\")\n",
    "    print('='*80)\n",
    "    \n",
    "    # Layer A: Extract signals from statistical model\n",
    "    signals = extract_signals(utterance)\n",
    "    \n",
    "    # Layer B: Apply neuro-symbolic rules\n",
    "    rules_output = apply_decision_rules(signals)\n",
    "    \n",
    "    # Transparent voting (for explainability only)\n",
    "    voting_output = weighted_voting(signals)\n",
    "    \n",
    "    # Structured output\n",
    "    output = {\n",
    "        'utterance': utterance,\n",
    "        'predicted_intent': rules_output['predicted_intent'],\n",
    "        'signals': {\n",
    "            'max_confidence': signals['max_confidence'],\n",
    "            'margin': signals['margin'],\n",
    "            'meaningful_tokens': signals['meaningful_tokens'],\n",
    "            'probabilities': signals['probabilities']\n",
    "        },\n",
    "        'triggered_rules': [\n",
    "            {\n",
    "                'rule_id': r['rule_id'],\n",
    "                'category': r['category'],\n",
    "                'priority': r['priority'],\n",
    "                'condition': r['condition'],\n",
    "                'value': r['signal_value']\n",
    "            } for r in rules_output['triggered_rules']\n",
    "        ],\n",
    "        'decision_state': rules_output['decision_state'],\n",
    "        'decision_reason': rules_output['decision_reason']\n",
    "    }\n",
    "    \n",
    "    # Print structured output\n",
    "    print(f\"\\nSTRUCTURED OUTPUT:\")\n",
    "    print(json.dumps(output, indent=2))\n",
    "    \n",
    "    # Print detailed breakdown\n",
    "    print(f\"\\n{'─'*80}\")\n",
    "    print(f\"LAYER A — STATISTICAL MODEL:\")\n",
    "    print(f\"  predicted_intent: {signals['predicted_intent']}\")\n",
    "    print(f\"  max_confidence: {signals['max_confidence']:.4f}\")\n",
    "    print(f\"  margin: {signals['margin']:.4f}\")\n",
    "    print(f\"  meaningful_tokens: {signals['meaningful_tokens']}\")\n",
    "    \n",
    "    print(f\"\\nLAYER B — NEURO-SYMBOLIC RULES:\")\n",
    "    if output['triggered_rules']:\n",
    "        print(f\"  Triggered: {len(output['triggered_rules'])} rule(s)\")\n",
    "        for rule in sorted(output['triggered_rules'], key=lambda r: r['priority'], reverse=True):\n",
    "            print(f\"    [P{rule['priority']}] {rule['rule_id']}: {rule['condition']} (value={rule['value']:.4f})\")\n",
    "    else:\n",
    "        print(f\"  No rules triggered - accepting model prediction\")\n",
    "    \n",
    "    print(f\"\\nTRANSPARENT VOTING (for reference):\")\n",
    "    print(f\"  vote_winner: {voting_output['vote_winner']}\")\n",
    "    for intent, score in sorted(voting_output['score_per_intent'].items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"    {intent}: {score:.4f}\")\n",
    "    \n",
    "    print(f\"\\n{'─'*80}\")\n",
    "    print(f\"FINAL DECISION:\")\n",
    "    print(f\"  predicted_intent: {output['predicted_intent']}\")\n",
    "    print(f\"  decision_state: {output['decision_state']}\")\n",
    "    print(f\"  decision_reason: {output['decision_reason']}\")\n",
    "    \n",
    "    if rules_output['triggered_rules']:\n",
    "        highest_priority_rule = sorted(rules_output['triggered_rules'], key=lambda r: r['priority'], reverse=True)[0]\n",
    "        print(f\"  ↳ Decision controlled by {highest_priority_rule['rule_id']} ({highest_priority_rule['category']})\")\n",
    "    else:\n",
    "        print(f\"  ↳ Model prediction accepted\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
